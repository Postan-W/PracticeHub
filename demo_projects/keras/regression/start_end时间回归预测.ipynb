{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,models,regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"start_end_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tool</th>\n",
       "      <th>r_c_p</th>\n",
       "      <th>batch_seq</th>\n",
       "      <th>lot_end-lot_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>1048</td>\n",
       "      <td>3886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1370</td>\n",
       "      <td>4561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>176</td>\n",
       "      <td>3108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>282</td>\n",
       "      <td>3971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>585</td>\n",
       "      <td>4629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tool  r_c_p  batch_seq  lot_end-lot_start\n",
       "0     0     68       1048               3886\n",
       "1     0     34       1370               4561\n",
       "2     0     34        176               3108\n",
       "3     0     29        282               3971\n",
       "4     0     37        585               4629"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)\n",
    "data.drop(columns=[\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x, y = np.split(data.values, (3,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   34  238]\n",
      " [   4   39  895]\n",
      " [   3   45 2944]\n",
      " ...\n",
      " [   4   58 2397]\n",
      " [   3   33 2827]\n",
      " [   1   29 1000]] [[2527]\n",
      " [2342]\n",
      " [2835]\n",
      " ...\n",
      " [1799]\n",
      " [2958]\n",
      " [3960]]\n"
     ]
    }
   ],
   "source": [
    "#这些离散特征值都是id，即代号，没有实际意义，不做数据标准化\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data,train_label,test_label = train_test_split(x, y, test_size=0.3)\n",
    "print(train_data,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 90us/step - loss: 5061784.7205 - mae: 1829.8610 - val_loss: 4541399.0393 - val_mae: 1705.7386\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 4600344.0837 - mae: 1731.7034 - val_loss: 4434682.0085 - val_mae: 1680.5067\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 1s 74us/step - loss: 4409039.7177 - mae: 1685.1930 - val_loss: 4199818.3392 - val_mae: 1635.7737\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 4287924.7962 - mae: 1655.0243 - val_loss: 4110852.0967 - val_mae: 1601.1863\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 4068954.2754 - mae: 1601.2820 - val_loss: 3816339.8731 - val_mae: 1536.1747\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3929115.0015 - mae: 1574.0881 - val_loss: 3608573.9001 - val_mae: 1486.1410\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3770955.5760 - mae: 1546.8827 - val_loss: 3765096.4312 - val_mae: 1494.4023\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3715949.6887 - mae: 1547.7869 - val_loss: 3416067.5060 - val_mae: 1485.9456\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3705878.6765 - mae: 1537.2559 - val_loss: 3502646.2214 - val_mae: 1494.3711\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3675301.4083 - mae: 1537.8752 - val_loss: 3374484.1036 - val_mae: 1489.3732\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3606554.6863 - mae: 1521.7283 - val_loss: 3380235.7180 - val_mae: 1475.6400\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3573909.9717 - mae: 1515.1589 - val_loss: 3426814.4642 - val_mae: 1476.4150\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 3503989.1638 - mae: 1500.8264 - val_loss: 3392726.5292 - val_mae: 1480.3859\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3512643.6281 - mae: 1505.0648 - val_loss: 3362194.1131 - val_mae: 1459.8376\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3506118.4030 - mae: 1502.5559 - val_loss: 3184488.8606 - val_mae: 1452.1913\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3456614.3005 - mae: 1493.0696 - val_loss: 3144767.0311 - val_mae: 1451.1912\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 3421513.8607 - mae: 1485.7002 - val_loss: 3134462.6011 - val_mae: 1418.0676\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 3348570.0777 - mae: 1463.0214 - val_loss: 3007558.6329 - val_mae: 1403.7214\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 3260498.1770 - mae: 1441.9590 - val_loss: 2916828.3533 - val_mae: 1377.6614\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 3200011.9145 - mae: 1431.8102 - val_loss: 2811248.0176 - val_mae: 1353.7446\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3022138.4863 - mae: 1380.8864 - val_loss: 2870371.7767 - val_mae: 1394.6860\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2861355.8288 - mae: 1333.5177 - val_loss: 2625700.5342 - val_mae: 1228.3427\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 2782996.1688 - mae: 1301.8704 - val_loss: 2378005.1729 - val_mae: 1151.1931\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2626524.5462 - mae: 1255.3981 - val_loss: 2354537.0077 - val_mae: 1157.5304\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 2552454.3876 - mae: 1234.1826 - val_loss: 2182793.6307 - val_mae: 1121.5562\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2496102.8614 - mae: 1215.6494 - val_loss: 2040919.9011 - val_mae: 1060.8795\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2467009.4656 - mae: 1203.9298 - val_loss: 2013162.8078 - val_mae: 1085.3741\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2447030.1908 - mae: 1201.6118 - val_loss: 1979738.5647 - val_mae: 1075.1863\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - ETA: 0s - loss: 2417875.7214 - mae: 1190.61 - 0s 63us/step - loss: 2413427.7519 - mae: 1190.1040 - val_loss: 2169194.5199 - val_mae: 1050.6045\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2443325.2053 - mae: 1202.3625 - val_loss: 1989373.3860 - val_mae: 1061.6136\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 2384225.9227 - mae: 1187.6484 - val_loss: 2153651.8819 - val_mae: 1039.4128\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2371186.7448 - mae: 1182.9905 - val_loss: 2817205.2497 - val_mae: 1405.7422\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 2378827.7737 - mae: 1185.4449 - val_loss: 1916158.8034 - val_mae: 1096.8740\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2331799.0030 - mae: 1170.5245 - val_loss: 2135419.5163 - val_mae: 1034.0758\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2297301.5639 - mae: 1163.8062 - val_loss: 2404807.1944 - val_mae: 1266.1405\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2316667.1474 - mae: 1164.5659 - val_loss: 1877708.3384 - val_mae: 1020.8980\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2278208.7574 - mae: 1150.8719 - val_loss: 1895522.7030 - val_mae: 996.3539\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2266198.5024 - mae: 1157.2567 - val_loss: 1776266.1533 - val_mae: 997.6777\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2226650.5721 - mae: 1143.7164 - val_loss: 1854403.4196 - val_mae: 1058.9111\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2230205.3888 - mae: 1140.5806 - val_loss: 1784575.6858 - val_mae: 974.8078\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2206273.8863 - mae: 1137.2234 - val_loss: 1832592.4179 - val_mae: 962.1905\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2177655.8083 - mae: 1122.8762 - val_loss: 1883751.7658 - val_mae: 969.4246\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2191396.4175 - mae: 1128.7761 - val_loss: 1666934.6807 - val_mae: 990.1357\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2148898.6815 - mae: 1117.5365 - val_loss: 1754550.5801 - val_mae: 934.3921\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2129079.1499 - mae: 1111.9916 - val_loss: 1748819.2037 - val_mae: 1059.0042\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2105619.3327 - mae: 1106.3484 - val_loss: 1937240.5835 - val_mae: 966.6707\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2077031.4262 - mae: 1093.8284 - val_loss: 1625005.8403 - val_mae: 960.3401\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2060317.3638 - mae: 1084.6736 - val_loss: 1579261.3072 - val_mae: 984.7159\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2048555.5028 - mae: 1078.5205 - val_loss: 1706660.4488 - val_mae: 1051.1188\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2015074.2384 - mae: 1069.6779 - val_loss: 1902625.7022 - val_mae: 953.7498\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2017443.7668 - mae: 1065.3656 - val_loss: 2434936.3763 - val_mae: 1305.4410\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 1s 79us/step - loss: 1983361.9398 - mae: 1051.7969 - val_loss: 1480792.9486 - val_mae: 939.5701\n",
      "Epoch 53/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 68us/step - loss: 1941872.5601 - mae: 1048.6305 - val_loss: 2255886.3131 - val_mae: 1239.8586\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1943612.1815 - mae: 1043.3572 - val_loss: 1385336.9058 - val_mae: 926.7646\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1899622.2057 - mae: 1033.5759 - val_loss: 1358214.4862 - val_mae: 914.4302\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1902262.9649 - mae: 1034.3208 - val_loss: 2325889.2605 - val_mae: 1095.3992\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1866288.6256 - mae: 1016.0185 - val_loss: 1911028.8654 - val_mae: 944.3033\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1804749.9760 - mae: 1000.7668 - val_loss: 1647006.5726 - val_mae: 834.3564\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1792966.1658 - mae: 993.3579 - val_loss: 1979528.3706 - val_mae: 1187.6533\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1793195.3482 - mae: 995.2704 - val_loss: 1989685.2937 - val_mae: 995.5709\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1736297.8866 - mae: 973.4588 - val_loss: 1365929.0532 - val_mae: 767.9257\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1760974.9293 - mae: 977.3580 - val_loss: 2395183.0905 - val_mae: 1316.6381\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1739035.3795 - mae: 974.0314 - val_loss: 1213512.3117 - val_mae: 857.4108\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1747646.1141 - mae: 969.3163 - val_loss: 1327059.2611 - val_mae: 736.4312\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1766189.9143 - mae: 973.3687 - val_loss: 1214179.3666 - val_mae: 740.6922\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1710427.7978 - mae: 953.7226 - val_loss: 1781667.9265 - val_mae: 907.1116\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1690440.3123 - mae: 953.4111 - val_loss: 1482017.8112 - val_mae: 1017.7890\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1671245.7248 - mae: 943.3940 - val_loss: 1154076.1170 - val_mae: 835.4007\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1687391.9307 - mae: 954.1148 - val_loss: 1146579.2342 - val_mae: 778.4613\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 1s 70us/step - loss: 1725051.1501 - mae: 962.7021 - val_loss: 2651617.9780 - val_mae: 1380.2352\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1735291.0914 - mae: 960.3469 - val_loss: 1125648.9561 - val_mae: 718.3434\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 1s 82us/step - loss: 1701927.4324 - mae: 952.4929 - val_loss: 1443775.3416 - val_mae: 1011.3484\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1713344.7222 - mae: 957.7154 - val_loss: 2328090.6875 - val_mae: 1291.8822\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1680017.8767 - mae: 945.3019 - val_loss: 2577211.4852 - val_mae: 1354.9606\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1682678.1460 - mae: 944.1957 - val_loss: 2576841.2849 - val_mae: 1363.6577\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1709273.9064 - mae: 952.1657 - val_loss: 1060848.9849 - val_mae: 740.0578\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 1s 81us/step - loss: 1699648.2836 - mae: 950.5025 - val_loss: 1264150.2608 - val_mae: 729.9797\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 1s 81us/step - loss: 1658026.1249 - mae: 945.1607 - val_loss: 2320089.6963 - val_mae: 1285.2561\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1667414.4979 - mae: 938.1065 - val_loss: 1405459.8284 - val_mae: 749.5770\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 1s 72us/step - loss: 1675575.8792 - mae: 938.1136 - val_loss: 1460770.0260 - val_mae: 778.0658\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 1s 70us/step - loss: 1697809.1489 - mae: 950.7643 - val_loss: 4478757.6149 - val_mae: 1788.8787\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1689441.6652 - mae: 944.3964 - val_loss: 1216803.5776 - val_mae: 855.0585\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1684820.2223 - mae: 945.1806 - val_loss: 1179701.9404 - val_mae: 755.4474\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1630142.0362 - mae: 930.0263 - val_loss: 1055468.6239 - val_mae: 746.1620\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1635692.8776 - mae: 931.6744 - val_loss: 1293819.1598 - val_mae: 710.2322\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1668156.6047 - mae: 940.7446 - val_loss: 1019582.2412 - val_mae: 730.5965\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1689827.2055 - mae: 945.8928 - val_loss: 1978482.2662 - val_mae: 983.3633\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1686873.6521 - mae: 947.3757 - val_loss: 1040298.1517 - val_mae: 737.3215\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1642555.1178 - mae: 933.9786 - val_loss: 2228455.8976 - val_mae: 1286.0779\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1643983.6978 - mae: 930.6863 - val_loss: 1343520.3404 - val_mae: 741.8020\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1653452.0581 - mae: 939.1027 - val_loss: 1859818.1154 - val_mae: 960.2336\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1646457.1600 - mae: 933.8500 - val_loss: 1059254.0832 - val_mae: 665.3663\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1641709.1142 - mae: 931.3296 - val_loss: 2078008.7965 - val_mae: 1034.1860\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1673760.2122 - mae: 941.8622 - val_loss: 1092864.6903 - val_mae: 807.9717\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1657256.9788 - mae: 941.5677 - val_loss: 1171657.4809 - val_mae: 813.0381\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1657943.4823 - mae: 939.4380 - val_loss: 1320960.9172 - val_mae: 716.2322\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1666572.1124 - mae: 945.5325 - val_loss: 1515843.8199 - val_mae: 769.0486\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1619729.6207 - mae: 920.6715 - val_loss: 2304702.0958 - val_mae: 1298.9818\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1662152.8535 - mae: 939.2797 - val_loss: 1697246.6129 - val_mae: 859.5160\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1643130.3406 - mae: 937.0314 - val_loss: 1848375.0664 - val_mae: 1165.1616\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1621024.8167 - mae: 929.1570 - val_loss: 1046003.9656 - val_mae: 802.4191\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1649916.4604 - mae: 930.2208 - val_loss: 1041527.7195 - val_mae: 812.3059\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 1653514.7631 - mae: 924.2039 - val_loss: 1048531.9853 - val_mae: 731.3845\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1621099.2371 - mae: 922.9845 - val_loss: 1108995.1067 - val_mae: 690.6708\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1658747.0838 - mae: 933.0765 - val_loss: 2078042.1453 - val_mae: 1017.3156\n",
      "Epoch 106/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 58us/step - loss: 1597687.1398 - mae: 918.9667 - val_loss: 1046758.2106 - val_mae: 700.4776\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1628230.0411 - mae: 925.1324 - val_loss: 1693179.7514 - val_mae: 856.7941\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1638278.0977 - mae: 927.2817 - val_loss: 1037440.4580 - val_mae: 690.3029\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1616756.1338 - mae: 922.4481 - val_loss: 1348335.1844 - val_mae: 979.3385\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1647537.5182 - mae: 932.4088 - val_loss: 2652559.9997 - val_mae: 1395.0131\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1615170.8285 - mae: 925.2651 - val_loss: 1538386.6489 - val_mae: 805.3608\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1591161.9154 - mae: 920.2815 - val_loss: 1285352.9018 - val_mae: 695.1163\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1626935.0364 - mae: 925.8093 - val_loss: 950407.6634 - val_mae: 717.6340\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1570483.5285 - mae: 915.7228 - val_loss: 1370661.0107 - val_mae: 717.5209\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1643489.8562 - mae: 927.1357 - val_loss: 1934086.4447 - val_mae: 961.7714\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1556851.8501 - mae: 897.8741 - val_loss: 1250536.5110 - val_mae: 691.8478\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1588232.0552 - mae: 919.9284 - val_loss: 1053688.6896 - val_mae: 684.4295\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1626608.0370 - mae: 917.1903 - val_loss: 965913.3351 - val_mae: 659.7631\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1596783.6946 - mae: 916.3433 - val_loss: 1039009.6556 - val_mae: 781.6673\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1576090.0350 - mae: 910.8360 - val_loss: 1274918.8618 - val_mae: 909.0028\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1580672.5026 - mae: 912.6907 - val_loss: 1322216.1432 - val_mae: 726.1771\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1607699.4916 - mae: 919.6422 - val_loss: 1246233.2451 - val_mae: 707.5640\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1597178.1856 - mae: 913.9785 - val_loss: 1039679.0962 - val_mae: 762.3599\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1613370.6443 - mae: 920.9796 - val_loss: 1024572.6375 - val_mae: 683.1143\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1569912.2954 - mae: 907.9722 - val_loss: 992017.9783 - val_mae: 656.7253\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1577169.5712 - mae: 910.6487 - val_loss: 1723377.3219 - val_mae: 888.8448\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1607311.1830 - mae: 917.0712 - val_loss: 1190669.0642 - val_mae: 854.8558\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1612257.5083 - mae: 917.6491 - val_loss: 978875.2898 - val_mae: 724.9177\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1556445.1715 - mae: 897.5822 - val_loss: 1164471.6451 - val_mae: 865.6625\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1621481.5898 - mae: 922.6854 - val_loss: 992017.6974 - val_mae: 781.3591\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1560586.4858 - mae: 899.7639 - val_loss: 985221.8246 - val_mae: 703.7909\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1575174.9085 - mae: 903.0593 - val_loss: 1632518.1126 - val_mae: 860.7106\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1555665.8061 - mae: 901.3578 - val_loss: 1570379.8172 - val_mae: 1039.6462\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1501759.5810 - mae: 894.9714 - val_loss: 1374321.0974 - val_mae: 727.2274\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1566634.1973 - mae: 908.4763 - val_loss: 1138493.1736 - val_mae: 694.1919\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1548906.3188 - mae: 901.3828 - val_loss: 1142017.7905 - val_mae: 675.9208\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1594278.4150 - mae: 912.3173 - val_loss: 1132023.7162 - val_mae: 844.7417\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1574077.9832 - mae: 907.8630 - val_loss: 1154339.3400 - val_mae: 683.8686\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1559850.8322 - mae: 901.2163 - val_loss: 1342318.9753 - val_mae: 970.1444\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1597920.6651 - mae: 908.4026 - val_loss: 1050502.1871 - val_mae: 676.9037\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1592928.8344 - mae: 911.3065 - val_loss: 987339.7843 - val_mae: 723.7793\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1520587.9574 - mae: 888.9711 - val_loss: 1473668.7013 - val_mae: 1014.8829\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1547368.2211 - mae: 897.6735 - val_loss: 938500.1390 - val_mae: 680.9677\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1564821.2484 - mae: 904.6734 - val_loss: 1021166.0033 - val_mae: 739.4342\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1559039.2468 - mae: 898.8330 - val_loss: 917570.4484 - val_mae: 708.1680\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1513504.9909 - mae: 888.4192 - val_loss: 1126920.3719 - val_mae: 653.3660\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1555018.3948 - mae: 897.3286 - val_loss: 1060549.4243 - val_mae: 651.0883\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1515954.8301 - mae: 884.9985 - val_loss: 1052117.6506 - val_mae: 806.4005\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1530031.4441 - mae: 891.8648 - val_loss: 1160774.1329 - val_mae: 669.4437\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1530370.5085 - mae: 892.1459 - val_loss: 1053970.1995 - val_mae: 677.8772\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1563984.4213 - mae: 896.7731 - val_loss: 1046854.0239 - val_mae: 711.5633\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1546790.9204 - mae: 900.7851 - val_loss: 1343704.0350 - val_mae: 732.4268\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1507560.1724 - mae: 885.8455 - val_loss: 1179502.5736 - val_mae: 653.7174\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1531874.9354 - mae: 889.2818 - val_loss: 1109125.4327 - val_mae: 683.9555\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1526423.7551 - mae: 888.9993 - val_loss: 1515965.0878 - val_mae: 1050.2211\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1516163.3737 - mae: 887.1561 - val_loss: 1013005.8678 - val_mae: 678.0834\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1516506.2964 - mae: 882.3896 - val_loss: 976683.5338 - val_mae: 695.8752\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 1s 75us/step - loss: 1534332.8354 - mae: 889.0598 - val_loss: 1369654.0527 - val_mae: 744.0809\n",
      "Epoch 159/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 60us/step - loss: 1516344.9279 - mae: 883.8452 - val_loss: 992881.6562 - val_mae: 628.4224\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1504928.8516 - mae: 878.0040 - val_loss: 2234553.3018 - val_mae: 1282.7095\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 79us/step - loss: 4855878.3025 - mae: 1785.5509 - val_loss: 4639978.8536 - val_mae: 1746.2203\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 4464677.2660 - mae: 1706.5518 - val_loss: 4417866.0974 - val_mae: 1701.2457\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 4363230.2951 - mae: 1675.6388 - val_loss: 4330404.5477 - val_mae: 1665.4573\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 4134962.5677 - mae: 1622.6434 - val_loss: 4144465.8712 - val_mae: 1601.3496\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3954301.7156 - mae: 1574.5181 - val_loss: 3846454.9523 - val_mae: 1526.1584\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3767833.1821 - mae: 1548.5864 - val_loss: 3790136.2349 - val_mae: 1513.4972\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3673412.7158 - mae: 1526.8970 - val_loss: 3549143.5195 - val_mae: 1511.0720\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3624759.0983 - mae: 1520.3749 - val_loss: 3495991.3876 - val_mae: 1554.3904\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3665910.6637 - mae: 1537.2874 - val_loss: 3455287.1869 - val_mae: 1506.3075\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3567921.7290 - mae: 1515.4929 - val_loss: 3400244.7754 - val_mae: 1510.0311\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3556514.3211 - mae: 1511.2002 - val_loss: 3456505.3731 - val_mae: 1490.7930\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3472556.0239 - mae: 1490.6729 - val_loss: 3507064.8279 - val_mae: 1488.8264\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3496117.4776 - mae: 1494.3267 - val_loss: 3308774.1608 - val_mae: 1495.8508\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3450837.4449 - mae: 1492.7542 - val_loss: 3315652.2572 - val_mae: 1474.2391\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3395632.8757 - mae: 1474.6420 - val_loss: 3232895.7500 - val_mae: 1468.9890\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3331116.6886 - mae: 1463.1198 - val_loss: 3272324.6341 - val_mae: 1433.6328\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3240182.9648 - mae: 1439.1550 - val_loss: 3080955.6222 - val_mae: 1418.5287\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3182758.2609 - mae: 1422.7124 - val_loss: 3098598.3499 - val_mae: 1380.4645\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3044789.2307 - mae: 1380.3928 - val_loss: 2843124.9595 - val_mae: 1336.5970\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2869793.4252 - mae: 1328.2375 - val_loss: 2616228.9400 - val_mae: 1302.1455\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2682082.2296 - mae: 1277.9996 - val_loss: 2542038.2563 - val_mae: 1221.2169\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2620991.7259 - mae: 1256.6333 - val_loss: 2838161.9163 - val_mae: 1221.5474\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2554832.7288 - mae: 1230.3503 - val_loss: 2378348.3717 - val_mae: 1156.2375\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2481039.1249 - mae: 1209.7185 - val_loss: 2271882.3463 - val_mae: 1151.5110\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2467601.9200 - mae: 1205.8246 - val_loss: 2196553.2616 - val_mae: 1148.7686\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2416895.6830 - mae: 1193.4496 - val_loss: 2740455.2604 - val_mae: 1177.6372\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2405611.1440 - mae: 1186.2439 - val_loss: 3407091.2657 - val_mae: 1544.7479\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2412488.9333 - mae: 1189.1431 - val_loss: 2152993.9461 - val_mae: 1160.7937\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2331439.8988 - mae: 1170.2242 - val_loss: 2740659.0013 - val_mae: 1174.9374\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 1s 82us/step - loss: 2304183.0762 - mae: 1162.2380 - val_loss: 2171558.0890 - val_mae: 1080.6366\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2324172.5514 - mae: 1164.5719 - val_loss: 2188866.8807 - val_mae: 1190.2301\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2315664.3830 - mae: 1165.3168 - val_loss: 2168663.5776 - val_mae: 1198.0745\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2294491.7409 - mae: 1163.2251 - val_loss: 2038142.8569 - val_mae: 1075.3719\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2289637.0339 - mae: 1155.5131 - val_loss: 2053745.6939 - val_mae: 1058.8457\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 1s 70us/step - loss: 2265289.9906 - mae: 1147.9165 - val_loss: 2019099.6052 - val_mae: 1102.9789\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2231659.1709 - mae: 1139.9330 - val_loss: 2338969.9675 - val_mae: 1077.4335\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2227671.3244 - mae: 1142.9935 - val_loss: 2042558.0035 - val_mae: 1039.9464\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2199227.3745 - mae: 1127.7816 - val_loss: 2311667.6850 - val_mae: 1065.7654\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2202794.8885 - mae: 1131.9116 - val_loss: 1888233.7778 - val_mae: 1080.0408\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2194829.4481 - mae: 1125.4728 - val_loss: 1875618.0421 - val_mae: 1091.5002\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2122887.6186 - mae: 1113.3232 - val_loss: 2160509.5083 - val_mae: 1040.3293\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2150706.2902 - mae: 1113.2283 - val_loss: 2286935.8026 - val_mae: 1055.5311\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2144035.4842 - mae: 1115.3639 - val_loss: 1854943.3481 - val_mae: 1083.4431\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2075722.3618 - mae: 1091.3788 - val_loss: 2130817.1627 - val_mae: 1219.7866\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 1s 73us/step - loss: 2069861.5315 - mae: 1092.2216 - val_loss: 1802621.0449 - val_mae: 1015.4293\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 2034704.9782 - mae: 1080.3112 - val_loss: 2410282.5188 - val_mae: 1304.3326\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 70us/step - loss: 2033318.7525 - mae: 1081.7705 - val_loss: 1992962.2720 - val_mae: 1165.5133\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 2003716.3411 - mae: 1065.6327 - val_loss: 2775763.9702 - val_mae: 1400.9993\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 1s 75us/step - loss: 1955660.8241 - mae: 1051.0433 - val_loss: 2182940.6241 - val_mae: 1234.2478\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 1s 70us/step - loss: 1993055.3540 - mae: 1066.1194 - val_loss: 2393555.6771 - val_mae: 1300.4674\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 1s 73us/step - loss: 1956379.0419 - mae: 1052.2805 - val_loss: 1704132.4996 - val_mae: 925.7932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1912682.6797 - mae: 1034.1096 - val_loss: 1629771.1168 - val_mae: 964.8921\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1876539.2848 - mae: 1026.3026 - val_loss: 1702146.2230 - val_mae: 907.6763\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1872994.8858 - mae: 1023.0367 - val_loss: 2120210.5897 - val_mae: 967.7733\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1838567.2866 - mae: 1011.1717 - val_loss: 1592376.4465 - val_mae: 857.2756\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 1s 70us/step - loss: 1829933.3781 - mae: 1000.0016 - val_loss: 2421682.7500 - val_mae: 1305.3510\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1791831.1013 - mae: 987.7034 - val_loss: 3375732.5747 - val_mae: 1526.6101\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1776367.6154 - mae: 989.2110 - val_loss: 2101116.3772 - val_mae: 948.2407\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1750427.1371 - mae: 978.6716 - val_loss: 2597055.5952 - val_mae: 1349.0364\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1794919.8856 - mae: 990.6103 - val_loss: 2756638.3788 - val_mae: 1373.3849\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1752451.0527 - mae: 971.6796 - val_loss: 1836725.6002 - val_mae: 1124.7545\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1741617.6311 - mae: 969.1648 - val_loss: 2006482.0628 - val_mae: 1169.1029\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1713385.2824 - mae: 964.0623 - val_loss: 2092706.4001 - val_mae: 1201.9713\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1708109.4616 - mae: 958.2852 - val_loss: 1403316.6621 - val_mae: 903.7132\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1713805.6055 - mae: 959.3954 - val_loss: 1664554.5686 - val_mae: 801.5250\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1726346.2918 - mae: 960.1766 - val_loss: 1895042.9432 - val_mae: 880.3729\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 1s 72us/step - loss: 1675274.9915 - mae: 955.0231 - val_loss: 1262453.4318 - val_mae: 776.9819\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1725982.4281 - mae: 956.7558 - val_loss: 1839279.2354 - val_mae: 855.8232\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1708909.0056 - mae: 955.9893 - val_loss: 2212625.4931 - val_mae: 1245.7662\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1702191.8552 - mae: 955.5782 - val_loss: 1661432.2123 - val_mae: 798.8257\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1715826.6261 - mae: 956.9603 - val_loss: 1322936.0974 - val_mae: 753.1583\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1687347.3625 - mae: 947.9451 - val_loss: 3771862.1734 - val_mae: 1629.2739\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1685885.4369 - mae: 947.3551 - val_loss: 1347259.5306 - val_mae: 888.1804\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1697886.9502 - mae: 949.0025 - val_loss: 1355848.8015 - val_mae: 910.4457\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1712319.1529 - mae: 954.4648 - val_loss: 1225488.6478 - val_mae: 837.5369\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1679260.0524 - mae: 946.3848 - val_loss: 1910432.9560 - val_mae: 883.7731\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1671876.7388 - mae: 939.0568 - val_loss: 1517686.4062 - val_mae: 1021.3958\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1620243.6880 - mae: 930.2397 - val_loss: 1354390.4408 - val_mae: 760.3001\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1720293.2097 - mae: 952.8433 - val_loss: 1316190.3486 - val_mae: 894.0973\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1659747.5727 - mae: 943.0533 - val_loss: 1713963.1359 - val_mae: 823.2042\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1662888.3906 - mae: 942.3940 - val_loss: 2277025.7502 - val_mae: 1047.7111\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1671572.9068 - mae: 941.7186 - val_loss: 1359192.0878 - val_mae: 877.4587\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1691162.7278 - mae: 947.8123 - val_loss: 1584813.3530 - val_mae: 775.5013\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1690629.9643 - mae: 945.1094 - val_loss: 1652455.8586 - val_mae: 797.6414\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1696886.4398 - mae: 947.5306 - val_loss: 1492122.6319 - val_mae: 1004.6488\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1670442.8887 - mae: 943.4293 - val_loss: 1436027.8233 - val_mae: 986.6160\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1678839.0020 - mae: 938.1045 - val_loss: 1449668.4537 - val_mae: 960.5817\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1669544.0165 - mae: 944.8187 - val_loss: 1556425.2037 - val_mae: 1045.9121\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1671862.2919 - mae: 941.3408 - val_loss: 1919202.0538 - val_mae: 903.6420\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1688174.4288 - mae: 941.4525 - val_loss: 1934201.1084 - val_mae: 1163.3612\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1642003.5819 - mae: 931.3554 - val_loss: 3860037.3543 - val_mae: 1644.3213\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1673233.0329 - mae: 933.1356 - val_loss: 2555643.2148 - val_mae: 1357.7186\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1631141.0641 - mae: 935.5854 - val_loss: 1299594.7170 - val_mae: 893.0529\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1650438.7497 - mae: 928.8714 - val_loss: 2416970.7222 - val_mae: 1093.6887\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1646529.3139 - mae: 928.4763 - val_loss: 1223150.8525 - val_mae: 749.0881\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1658796.2654 - mae: 933.2664 - val_loss: 1342800.0001 - val_mae: 723.9265\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1645452.3922 - mae: 926.1088 - val_loss: 1653306.2179 - val_mae: 798.0231\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1661474.4148 - mae: 935.4144 - val_loss: 1260991.4176 - val_mae: 867.1587\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1635876.9525 - mae: 936.1900 - val_loss: 1585750.0248 - val_mae: 802.3283\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1650183.8361 - mae: 933.6415 - val_loss: 1136326.2916 - val_mae: 755.3610\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1633196.4443 - mae: 931.0837 - val_loss: 1749129.0443 - val_mae: 841.4243\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1607539.0761 - mae: 919.8759 - val_loss: 1511385.2819 - val_mae: 1021.1097\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1662319.2837 - mae: 934.4554 - val_loss: 1763189.5009 - val_mae: 1119.5856\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1634450.3200 - mae: 925.3867 - val_loss: 1404325.1588 - val_mae: 736.8303\n",
      "Epoch 105/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 60us/step - loss: 1648397.8071 - mae: 930.0650 - val_loss: 1326969.6145 - val_mae: 841.2955\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1637651.8331 - mae: 934.7311 - val_loss: 2143180.2007 - val_mae: 1236.7793\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1600911.5681 - mae: 925.1472 - val_loss: 1347713.8643 - val_mae: 949.1565\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1615877.5926 - mae: 920.1943 - val_loss: 1886166.9024 - val_mae: 893.3104\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 1s 81us/step - loss: 1628419.9345 - mae: 928.3785 - val_loss: 1166700.2536 - val_mae: 789.0171\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1602380.2962 - mae: 917.8042 - val_loss: 1221857.1197 - val_mae: 727.9713\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 70us/step - loss: 1601437.3888 - mae: 918.0062 - val_loss: 1167603.5528 - val_mae: 759.2852\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1637227.5761 - mae: 931.1804 - val_loss: 1449141.8271 - val_mae: 727.7997\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1628126.8476 - mae: 927.2471 - val_loss: 2450726.5688 - val_mae: 1112.1564\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1630232.4052 - mae: 926.7866 - val_loss: 1194254.3634 - val_mae: 718.5419\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1591328.1952 - mae: 907.1183 - val_loss: 1234223.7619 - val_mae: 887.8180\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 1s 72us/step - loss: 1644024.2074 - mae: 924.5015 - val_loss: 1412219.0215 - val_mae: 723.0302\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1591820.3285 - mae: 915.8475 - val_loss: 1626768.8208 - val_mae: 803.5084\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1602863.9985 - mae: 915.8461 - val_loss: 1293123.9013 - val_mae: 717.4936\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1640371.6171 - mae: 931.4582 - val_loss: 1235045.7683 - val_mae: 804.2346\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1567676.6040 - mae: 906.9475 - val_loss: 1188158.9363 - val_mae: 829.0153\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1616856.0771 - mae: 920.2516 - val_loss: 1538192.3935 - val_mae: 758.9673\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1623914.4579 - mae: 922.8848 - val_loss: 1226882.7367 - val_mae: 844.2040\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1585233.0403 - mae: 916.3849 - val_loss: 1207206.3227 - val_mae: 733.5450\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 1609428.8315 - mae: 924.7108 - val_loss: 1187803.7808 - val_mae: 753.2342\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1602094.6104 - mae: 918.2610 - val_loss: 1252691.1710 - val_mae: 701.5206\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1579061.0260 - mae: 915.3663 - val_loss: 1119150.0334 - val_mae: 782.8624\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1555844.2498 - mae: 904.5480 - val_loss: 1312774.7650 - val_mae: 704.6811\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1567895.5061 - mae: 906.3837 - val_loss: 2004644.7636 - val_mae: 947.2993\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1581838.0675 - mae: 910.8395 - val_loss: 2380207.7294 - val_mae: 1092.4882\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1597992.3261 - mae: 909.0574 - val_loss: 1748491.3081 - val_mae: 1116.9308\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1580801.6149 - mae: 910.6926 - val_loss: 1528667.4012 - val_mae: 753.7813\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1582485.4057 - mae: 908.9798 - val_loss: 1501144.0592 - val_mae: 761.9504\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1602189.4088 - mae: 914.3076 - val_loss: 3184338.6822 - val_mae: 1498.6086\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1578999.8127 - mae: 911.6258 - val_loss: 1438197.3052 - val_mae: 732.7108\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1568537.6106 - mae: 910.0250 - val_loss: 1140501.4538 - val_mae: 699.0087\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 1573836.0913 - mae: 907.4484 - val_loss: 2024118.4341 - val_mae: 928.0131\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 1s 85us/step - loss: 1578445.9594 - mae: 909.0779 - val_loss: 1400526.7410 - val_mae: 760.5936\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1576038.3578 - mae: 902.5652 - val_loss: 1993772.3147 - val_mae: 1184.3589\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1597893.6765 - mae: 917.2162 - val_loss: 1880403.8960 - val_mae: 1165.9780\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1552173.3083 - mae: 902.0309 - val_loss: 1219099.0916 - val_mae: 888.8759\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1566359.9270 - mae: 914.5096 - val_loss: 1345968.8103 - val_mae: 943.8585\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1583378.3265 - mae: 907.9453 - val_loss: 1143992.9895 - val_mae: 825.2020\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1530646.8129 - mae: 900.1449 - val_loss: 1223415.5342 - val_mae: 867.7643\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1545644.8002 - mae: 900.8312 - val_loss: 1458989.4582 - val_mae: 997.8022\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1584632.6203 - mae: 910.3538 - val_loss: 1882675.4842 - val_mae: 862.1031\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1577760.5959 - mae: 908.2225 - val_loss: 2124642.1365 - val_mae: 990.0800\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1547573.5668 - mae: 896.5771 - val_loss: 4037742.6470 - val_mae: 1681.0004\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1589417.4689 - mae: 906.3901 - val_loss: 1434433.1926 - val_mae: 751.1317\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1602954.2686 - mae: 921.2178 - val_loss: 1567947.3754 - val_mae: 782.2397\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1542610.7743 - mae: 892.6359 - val_loss: 2321159.1124 - val_mae: 1289.9058\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1567667.2018 - mae: 905.1968 - val_loss: 1183995.0846 - val_mae: 808.9137\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1531424.7787 - mae: 895.0107 - val_loss: 1273973.7407 - val_mae: 700.5778\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1541654.2528 - mae: 892.0297 - val_loss: 1303408.2353 - val_mae: 774.8799\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1535713.6135 - mae: 888.3015 - val_loss: 1147134.9623 - val_mae: 720.2363\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1555545.2867 - mae: 898.2726 - val_loss: 1269125.5815 - val_mae: 905.4636\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1556449.3368 - mae: 896.8672 - val_loss: 1128987.9494 - val_mae: 798.2745\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1561568.0945 - mae: 897.7739 - val_loss: 1225118.0414 - val_mae: 721.3368\n",
      "Epoch 158/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 58us/step - loss: 1492253.9567 - mae: 877.9709 - val_loss: 1382791.1654 - val_mae: 965.8806\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1539712.2299 - mae: 890.5215 - val_loss: 1255827.6072 - val_mae: 698.0304\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1541535.4504 - mae: 893.8107 - val_loss: 1287550.8904 - val_mae: 915.9764\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 81us/step - loss: 5091498.9104 - mae: 1834.4943 - val_loss: 3653399.3646 - val_mae: 1536.7478\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 4619579.5801 - mae: 1729.8099 - val_loss: 3553661.4303 - val_mae: 1520.4752\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 4403271.2021 - mae: 1685.7189 - val_loss: 3580847.4359 - val_mae: 1497.2166\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 4244941.7556 - mae: 1638.3960 - val_loss: 3274776.3656 - val_mae: 1444.2224\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 1s 74us/step - loss: 4011112.6020 - mae: 1584.4460 - val_loss: 3150747.5735 - val_mae: 1390.4216\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 1s 84us/step - loss: 3798366.9271 - mae: 1551.0038 - val_loss: 3058734.6928 - val_mae: 1396.8831\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3764497.5189 - mae: 1548.5895 - val_loss: 3016395.5986 - val_mae: 1442.2728\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 3699643.3706 - mae: 1541.5886 - val_loss: 2967280.0273 - val_mae: 1401.0089\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3686637.8093 - mae: 1540.6990 - val_loss: 2942976.1416 - val_mae: 1377.3478\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3620587.4376 - mae: 1528.7089 - val_loss: 2999184.6212 - val_mae: 1389.0992\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3613528.3005 - mae: 1520.8798 - val_loss: 2898484.5433 - val_mae: 1388.6519\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3620032.7322 - mae: 1526.4158 - val_loss: 2918625.8279 - val_mae: 1357.9248\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3564842.4699 - mae: 1516.6208 - val_loss: 2808947.1661 - val_mae: 1379.2926\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3493017.6135 - mae: 1506.0441 - val_loss: 2814103.4454 - val_mae: 1344.8579\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3515269.6136 - mae: 1500.3075 - val_loss: 2769333.3562 - val_mae: 1382.5067\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3492905.0596 - mae: 1500.6121 - val_loss: 2686185.0682 - val_mae: 1354.3135\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3417797.7077 - mae: 1478.0707 - val_loss: 2756037.1391 - val_mae: 1300.3174\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3281584.8964 - mae: 1447.3092 - val_loss: 2530523.8411 - val_mae: 1291.8568\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3207955.8146 - mae: 1432.5675 - val_loss: 2503230.9073 - val_mae: 1221.2517\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3064097.2163 - mae: 1387.6620 - val_loss: 2270449.9362 - val_mae: 1217.1796\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2935255.9433 - mae: 1349.1461 - val_loss: 2126119.3067 - val_mae: 1115.3425\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2796328.0372 - mae: 1310.5084 - val_loss: 1978817.0826 - val_mae: 1103.5399\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2681244.8161 - mae: 1269.7941 - val_loss: 1851769.8494 - val_mae: 1049.1147\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2541188.0061 - mae: 1229.9695 - val_loss: 1822957.0749 - val_mae: 1036.5498\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2517975.3296 - mae: 1223.2067 - val_loss: 1800171.8452 - val_mae: 1047.3923\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2475277.8092 - mae: 1209.6794 - val_loss: 1860159.5267 - val_mae: 1045.4285\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2507030.8108 - mae: 1220.2947 - val_loss: 1690861.5727 - val_mae: 1007.2746\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2465483.8983 - mae: 1206.1339 - val_loss: 1785671.8785 - val_mae: 947.9338\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2467053.9603 - mae: 1208.2889 - val_loss: 1867069.4808 - val_mae: 1130.1836\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2438205.3522 - mae: 1197.2821 - val_loss: 1851263.2913 - val_mae: 1002.5256\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2320741.3527 - mae: 1170.3362 - val_loss: 1630715.0166 - val_mae: 976.3723\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2409614.3925 - mae: 1185.6333 - val_loss: 1803483.0835 - val_mae: 964.4716\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2370741.8351 - mae: 1176.2472 - val_loss: 1787441.5704 - val_mae: 1065.5205\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2356239.9085 - mae: 1171.7213 - val_loss: 1643421.2456 - val_mae: 987.7682\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2373270.3762 - mae: 1175.6702 - val_loss: 2224597.2786 - val_mae: 1231.9220\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2334959.8296 - mae: 1174.7645 - val_loss: 1565602.5248 - val_mae: 971.7963\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2265434.0543 - mae: 1154.1885 - val_loss: 1686638.4629 - val_mae: 943.2585\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2300703.3213 - mae: 1160.0546 - val_loss: 1958587.0350 - val_mae: 960.6902\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2281683.4297 - mae: 1160.8363 - val_loss: 1561738.1242 - val_mae: 998.0272\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2247458.4370 - mae: 1139.4183 - val_loss: 2773790.1269 - val_mae: 1411.0702\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2221410.2609 - mae: 1140.4268 - val_loss: 1607339.6448 - val_mae: 893.5930\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2191854.1141 - mae: 1130.7671 - val_loss: 1506903.4454 - val_mae: 879.7961\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2170129.2811 - mae: 1115.8010 - val_loss: 1445586.4661 - val_mae: 941.1107\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2160135.4709 - mae: 1115.3275 - val_loss: 1644062.4265 - val_mae: 1078.0450\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2191108.5197 - mae: 1126.0271 - val_loss: 1391756.1084 - val_mae: 878.6740\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2116248.7840 - mae: 1105.0962 - val_loss: 2685228.7657 - val_mae: 1399.8118\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2103841.7689 - mae: 1102.4795 - val_loss: 1824325.0807 - val_mae: 930.8956\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2105934.4109 - mae: 1096.7252 - val_loss: 1365627.6803 - val_mae: 870.5449\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2070567.7810 - mae: 1090.1591 - val_loss: 1557278.0820 - val_mae: 850.6792\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2040309.5585 - mae: 1075.8324 - val_loss: 2868915.3285 - val_mae: 1440.3628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1987357.1608 - mae: 1056.3760 - val_loss: 1446119.0818 - val_mae: 813.7870\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2024480.3497 - mae: 1070.9967 - val_loss: 1254613.1589 - val_mae: 824.0278\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1989068.9483 - mae: 1056.2275 - val_loss: 3357676.7519 - val_mae: 1548.0773\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1942019.4824 - mae: 1048.7671 - val_loss: 1252323.2131 - val_mae: 770.6044\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1924269.0390 - mae: 1031.8788 - val_loss: 1647719.4655 - val_mae: 1072.4581\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1888885.1918 - mae: 1026.4016 - val_loss: 1316559.7648 - val_mae: 937.4553\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1844783.6273 - mae: 1016.7600 - val_loss: 1588586.1997 - val_mae: 1061.2291\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1815473.7415 - mae: 1005.1849 - val_loss: 1133415.9963 - val_mae: 735.1580\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1787695.0312 - mae: 987.8196 - val_loss: 1814203.1699 - val_mae: 1131.1364\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1803047.4798 - mae: 998.0379 - val_loss: 1181134.1052 - val_mae: 887.5346\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1723245.8688 - mae: 969.5042 - val_loss: 1080724.3882 - val_mae: 701.3510\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1812373.1547 - mae: 991.4517 - val_loss: 1074957.9424 - val_mae: 805.8296\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1784484.8126 - mae: 985.9917 - val_loss: 1737380.1404 - val_mae: 1111.2968\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1755916.1862 - mae: 972.2725 - val_loss: 1703469.6043 - val_mae: 1104.2620\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1726344.8139 - mae: 968.4676 - val_loss: 981559.6864 - val_mae: 701.9358\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1730369.8053 - mae: 963.8428 - val_loss: 1113118.0814 - val_mae: 656.7653\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1713354.0476 - mae: 955.6606 - val_loss: 1757024.7112 - val_mae: 903.0546\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1755536.3180 - mae: 971.3060 - val_loss: 949804.6384 - val_mae: 694.6869\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1715899.0660 - mae: 960.7740 - val_loss: 1046449.1649 - val_mae: 820.0795\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1742220.6129 - mae: 965.9789 - val_loss: 1206479.0068 - val_mae: 900.6426\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1729052.5290 - mae: 963.6070 - val_loss: 1458697.4683 - val_mae: 796.0728\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1715098.0194 - mae: 961.8544 - val_loss: 2133777.9786 - val_mae: 1260.5236\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1712458.6250 - mae: 961.1542 - val_loss: 3268844.5038 - val_mae: 1544.9470\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1773942.6940 - mae: 971.0825 - val_loss: 1191257.6625 - val_mae: 668.9189\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1717166.9136 - mae: 958.0858 - val_loss: 1324940.9138 - val_mae: 949.0674\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1734937.5891 - mae: 962.8691 - val_loss: 1081310.8999 - val_mae: 692.1754\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1691872.8835 - mae: 948.5584 - val_loss: 1599816.1360 - val_mae: 1078.6844\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1716565.4660 - mae: 952.7407 - val_loss: 2075944.5143 - val_mae: 1041.1936\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1687284.1997 - mae: 949.3824 - val_loss: 984271.7196 - val_mae: 651.4268\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1753430.2478 - mae: 966.4609 - val_loss: 1663297.4852 - val_mae: 825.0849\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1669986.7045 - mae: 940.6606 - val_loss: 1400767.0907 - val_mae: 737.1269\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1687261.3471 - mae: 945.5578 - val_loss: 908791.8473 - val_mae: 640.6411\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1688776.3877 - mae: 948.5792 - val_loss: 1269443.6071 - val_mae: 693.3320\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1656344.3167 - mae: 942.2980 - val_loss: 994042.7911 - val_mae: 624.4932\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1714797.5570 - mae: 959.0546 - val_loss: 954168.5069 - val_mae: 667.6826\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1692298.0989 - mae: 950.3734 - val_loss: 1449841.3028 - val_mae: 775.0919\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1688438.3468 - mae: 945.0490 - val_loss: 1003735.9820 - val_mae: 747.7880\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1686550.8750 - mae: 948.9899 - val_loss: 2060001.2990 - val_mae: 1027.6262\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1674267.8814 - mae: 939.8246 - val_loss: 1052634.2827 - val_mae: 621.4089\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1682599.0686 - mae: 950.6538 - val_loss: 2074490.2286 - val_mae: 1234.5607\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1668415.7514 - mae: 939.3518 - val_loss: 2630420.1966 - val_mae: 1364.4962\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1672219.7482 - mae: 943.2052 - val_loss: 1218667.1994 - val_mae: 905.2495\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1625721.6415 - mae: 928.3303 - val_loss: 1477688.2354 - val_mae: 807.5992\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1735417.9468 - mae: 965.2654 - val_loss: 1426084.4932 - val_mae: 774.3690\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1663515.5533 - mae: 937.5696 - val_loss: 963318.3621 - val_mae: 721.7575\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1694636.7864 - mae: 945.8607 - val_loss: 1041968.2174 - val_mae: 732.6627\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 1s 77us/step - loss: 1686421.1112 - mae: 944.0026 - val_loss: 1394893.1267 - val_mae: 773.9738\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 1s 85us/step - loss: 1682439.3452 - mae: 944.3201 - val_loss: 1076909.4087 - val_mae: 619.0091\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 1s 84us/step - loss: 1642891.8755 - mae: 933.8364 - val_loss: 1068214.2724 - val_mae: 647.5901\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 1s 77us/step - loss: 1672931.5966 - mae: 949.7327 - val_loss: 1083181.7074 - val_mae: 846.9473\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1646668.0177 - mae: 936.6658 - val_loss: 955071.0819 - val_mae: 719.3331\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1733745.6550 - mae: 958.8229 - val_loss: 918770.2275 - val_mae: 676.9153\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 1s 79us/step - loss: 1693540.9965 - mae: 949.1122 - val_loss: 1540018.0872 - val_mae: 806.6693\n",
      "Epoch 104/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 74us/step - loss: 1632812.4433 - mae: 936.0818 - val_loss: 1160381.9724 - val_mae: 665.3102\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1684017.8708 - mae: 942.0033 - val_loss: 1162354.5512 - val_mae: 893.5707\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1644690.7301 - mae: 932.1289 - val_loss: 1426099.5817 - val_mae: 769.8507\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1624217.6401 - mae: 929.6855 - val_loss: 912067.7434 - val_mae: 606.1243\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1677480.0592 - mae: 940.1792 - val_loss: 3601779.8072 - val_mae: 1647.8987\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1629508.0922 - mae: 922.0167 - val_loss: 987525.0345 - val_mae: 777.4661\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1669845.6757 - mae: 945.0269 - val_loss: 2649847.0405 - val_mae: 1368.8779\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1633672.4211 - mae: 925.6560 - val_loss: 913230.4157 - val_mae: 628.1699\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1622023.3011 - mae: 927.8203 - val_loss: 994597.5065 - val_mae: 607.2972\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1632308.9056 - mae: 929.7708 - val_loss: 909545.8881 - val_mae: 646.0251\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1636865.1288 - mae: 929.0975 - val_loss: 1074584.0736 - val_mae: 623.9628\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1662017.1893 - mae: 933.9362 - val_loss: 938971.7595 - val_mae: 717.5667\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1623931.2107 - mae: 924.4004 - val_loss: 935385.6135 - val_mae: 627.4106\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1661360.8370 - mae: 937.0057 - val_loss: 1365090.2663 - val_mae: 749.3503\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1637467.4766 - mae: 928.1573 - val_loss: 1362836.6159 - val_mae: 979.4502\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1665931.7180 - mae: 939.2091 - val_loss: 1037342.8668 - val_mae: 626.5873\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1619030.9300 - mae: 924.0524 - val_loss: 1467187.3504 - val_mae: 776.3165\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1657794.5256 - mae: 935.2558 - val_loss: 1089427.2674 - val_mae: 635.6180\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1619749.9368 - mae: 924.6559 - val_loss: 954998.9684 - val_mae: 722.1050\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1639458.9607 - mae: 930.2922 - val_loss: 5829881.5829 - val_mae: 2081.6528\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 1625722.3094 - mae: 929.5335 - val_loss: 1368921.6482 - val_mae: 744.1190\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 1s 70us/step - loss: 1640177.6895 - mae: 926.3472 - val_loss: 3672695.5113 - val_mae: 1673.1738\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1648295.2974 - mae: 928.6036 - val_loss: 1430611.4623 - val_mae: 745.4335\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 1s 70us/step - loss: 1637345.5738 - mae: 926.6500 - val_loss: 1350279.1624 - val_mae: 731.2961\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1575650.5889 - mae: 908.9585 - val_loss: 1393926.3659 - val_mae: 1015.9904\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1619252.4407 - mae: 928.8568 - val_loss: 1125161.3468 - val_mae: 645.3815\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1604703.1084 - mae: 924.6617 - val_loss: 2842756.5044 - val_mae: 1463.2836\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1575492.4231 - mae: 910.6962 - val_loss: 942990.6238 - val_mae: 760.4120\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1602281.7673 - mae: 916.7582 - val_loss: 951600.3782 - val_mae: 692.7051\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1607183.2290 - mae: 917.2102 - val_loss: 894001.8091 - val_mae: 700.4404\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1607533.2907 - mae: 924.3740 - val_loss: 1071830.1276 - val_mae: 645.1790\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1608242.4005 - mae: 920.4632 - val_loss: 1051502.4403 - val_mae: 636.3813\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1619024.7252 - mae: 923.0013 - val_loss: 4358444.5182 - val_mae: 1815.2522\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1614917.9095 - mae: 924.2296 - val_loss: 1546333.3511 - val_mae: 791.5923\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1603719.1046 - mae: 917.9320 - val_loss: 1504950.0638 - val_mae: 1054.8242\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1587061.4569 - mae: 915.9688 - val_loss: 2715961.5879 - val_mae: 1448.1376\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1603602.2556 - mae: 924.1300 - val_loss: 2103460.7070 - val_mae: 1251.8347\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1586154.6297 - mae: 911.9481 - val_loss: 871281.2016 - val_mae: 686.0464\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1528816.6893 - mae: 890.9075 - val_loss: 1966371.3160 - val_mae: 1192.0804\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1586056.6680 - mae: 915.1061 - val_loss: 1622710.4011 - val_mae: 1087.8588\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1626296.9819 - mae: 922.6937 - val_loss: 1712370.2637 - val_mae: 901.6592\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1602354.8844 - mae: 916.6476 - val_loss: 928633.1701 - val_mae: 771.9259\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1564545.7755 - mae: 904.6055 - val_loss: 1147311.0124 - val_mae: 864.3144\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1559933.4133 - mae: 903.7229 - val_loss: 857026.8435 - val_mae: 654.9460\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1570306.6215 - mae: 909.0406 - val_loss: 919219.4794 - val_mae: 592.3689\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1573206.9191 - mae: 905.9313 - val_loss: 857713.8810 - val_mae: 635.2290\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1567570.7676 - mae: 906.7947 - val_loss: 1066748.0225 - val_mae: 605.6813\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1555601.0260 - mae: 900.4333 - val_loss: 1065779.8686 - val_mae: 625.4854\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1617841.2505 - mae: 921.8216 - val_loss: 1299520.2775 - val_mae: 697.2191\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1579365.4347 - mae: 907.1489 - val_loss: 858729.4438 - val_mae: 641.8336\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1534956.0323 - mae: 899.2704 - val_loss: 1036225.1559 - val_mae: 803.4102\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1620868.9992 - mae: 917.2863 - val_loss: 868305.4985 - val_mae: 592.1216\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1568759.1271 - mae: 902.3838 - val_loss: 902954.0462 - val_mae: 634.6053\n",
      "Epoch 157/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 57us/step - loss: 1575506.2947 - mae: 907.7219 - val_loss: 1138205.4626 - val_mae: 639.7187\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1604240.9619 - mae: 909.5723 - val_loss: 1670419.5628 - val_mae: 1106.2457\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1607054.8435 - mae: 914.9435 - val_loss: 1679999.0716 - val_mae: 1111.1445\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1574313.8481 - mae: 910.7335 - val_loss: 1608448.2644 - val_mae: 1087.1465\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 86us/step - loss: 5058630.1054 - mae: 1829.0781 - val_loss: 4350990.5861 - val_mae: 1682.9379\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 4550255.6852 - mae: 1711.7175 - val_loss: 4423727.6834 - val_mae: 1682.5247\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 1s 78us/step - loss: 4434626.6395 - mae: 1690.2579 - val_loss: 4102738.4347 - val_mae: 1625.5391\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 4282865.2137 - mae: 1658.5143 - val_loss: 3997740.8901 - val_mae: 1587.7186\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 4108465.3949 - mae: 1611.4354 - val_loss: 3905051.6156 - val_mae: 1545.4146\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 1s 85us/step - loss: 3885754.5704 - mae: 1556.3195 - val_loss: 3678759.8467 - val_mae: 1500.3837\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 3754312.5488 - mae: 1541.6154 - val_loss: 3624426.8040 - val_mae: 1513.3931\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3689384.2663 - mae: 1545.2714 - val_loss: 3470040.9422 - val_mae: 1506.9579\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3627041.5343 - mae: 1524.9788 - val_loss: 3437651.3232 - val_mae: 1503.4033\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3594151.9990 - mae: 1522.9269 - val_loss: 3595853.6520 - val_mae: 1500.5111\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3586182.8060 - mae: 1521.3235 - val_loss: 3642832.3398 - val_mae: 1511.7301\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3511722.0041 - mae: 1506.2434 - val_loss: 3433142.7792 - val_mae: 1499.8931\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3502300.3301 - mae: 1505.8577 - val_loss: 3381705.2327 - val_mae: 1489.4222\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3484985.6714 - mae: 1499.9509 - val_loss: 3460300.6297 - val_mae: 1481.6914\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3391554.5352 - mae: 1476.8787 - val_loss: 3484147.4510 - val_mae: 1467.6903\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3392928.8056 - mae: 1475.7668 - val_loss: 3328451.2550 - val_mae: 1443.1228\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3344758.3693 - mae: 1467.4581 - val_loss: 3192777.7252 - val_mae: 1429.7439\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 1s 72us/step - loss: 3233932.5623 - mae: 1439.9822 - val_loss: 3485909.2613 - val_mae: 1418.5564\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3126513.4496 - mae: 1402.4999 - val_loss: 2851382.2522 - val_mae: 1366.8153\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 3041338.2585 - mae: 1382.0558 - val_loss: 3148863.9673 - val_mae: 1329.9540\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2850452.6267 - mae: 1323.5657 - val_loss: 2532525.1542 - val_mae: 1248.4270\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2716337.1627 - mae: 1283.1473 - val_loss: 2526353.9403 - val_mae: 1183.3296\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2622652.3972 - mae: 1252.4738 - val_loss: 2360308.8794 - val_mae: 1184.3395\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2524333.6360 - mae: 1226.3090 - val_loss: 2223132.6869 - val_mae: 1152.3943\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2434787.8773 - mae: 1194.6478 - val_loss: 2129498.0402 - val_mae: 1120.0079\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2475153.4046 - mae: 1205.7869 - val_loss: 2536867.2500 - val_mae: 1131.5504\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2405324.3933 - mae: 1186.7655 - val_loss: 2441394.3489 - val_mae: 1111.1152\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2364152.7552 - mae: 1176.2609 - val_loss: 2122520.3775 - val_mae: 1109.8162\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2371426.2117 - mae: 1179.2000 - val_loss: 2065966.9843 - val_mae: 1188.5643\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2398812.5812 - mae: 1180.8313 - val_loss: 1958743.2368 - val_mae: 1134.3372\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2352668.6340 - mae: 1169.3776 - val_loss: 1966191.6237 - val_mae: 1094.9706\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2334962.4179 - mae: 1162.1212 - val_loss: 2289808.3395 - val_mae: 1272.1846\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2312698.8423 - mae: 1160.2455 - val_loss: 2390190.9042 - val_mae: 1084.5762\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2286758.5262 - mae: 1159.2863 - val_loss: 1923421.2293 - val_mae: 1086.7168\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2287632.7646 - mae: 1153.1638 - val_loss: 2191798.7016 - val_mae: 1249.6632\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2297538.0787 - mae: 1161.8203 - val_loss: 1925841.6589 - val_mae: 1052.8983\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2247721.4904 - mae: 1147.1462 - val_loss: 1883027.6113 - val_mae: 1064.5314\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2232918.5265 - mae: 1137.9133 - val_loss: 2219974.5181 - val_mae: 1039.7490\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2224603.5034 - mae: 1134.1127 - val_loss: 1934362.5214 - val_mae: 1046.6182\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2224407.1603 - mae: 1133.1852 - val_loss: 2235005.9827 - val_mae: 1266.3148\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2199325.5435 - mae: 1131.6769 - val_loss: 1763832.4761 - val_mae: 1068.9492\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2186698.5378 - mae: 1123.1527 - val_loss: 2234573.8788 - val_mae: 1030.2811\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2168416.8891 - mae: 1116.9966 - val_loss: 1727743.7092 - val_mae: 1035.1699\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2126564.7251 - mae: 1106.8370 - val_loss: 2637873.5236 - val_mae: 1363.6462\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2108311.2937 - mae: 1108.5341 - val_loss: 2076931.1490 - val_mae: 1035.9381\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2096988.8519 - mae: 1099.9542 - val_loss: 1765525.0699 - val_mae: 1074.3700\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2108805.3785 - mae: 1098.1241 - val_loss: 1710971.0426 - val_mae: 1028.0048\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2073941.0939 - mae: 1092.8684 - val_loss: 1621643.3079 - val_mae: 986.2993\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2024974.5945 - mae: 1074.8698 - val_loss: 2210884.7268 - val_mae: 1063.2740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2007911.5514 - mae: 1063.1152 - val_loss: 1592614.4292 - val_mae: 944.1215\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1984272.8725 - mae: 1063.7878 - val_loss: 2582904.3050 - val_mae: 1117.6827\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1980702.9200 - mae: 1060.8024 - val_loss: 1598332.8394 - val_mae: 950.1840\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1925209.6339 - mae: 1045.8748 - val_loss: 1745381.0606 - val_mae: 1093.6865\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1889334.7886 - mae: 1033.9731 - val_loss: 1612378.6159 - val_mae: 1042.9524\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1869231.9504 - mae: 1021.4136 - val_loss: 2280599.0421 - val_mae: 1274.3608\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1893696.7536 - mae: 1025.9489 - val_loss: 2130100.8094 - val_mae: 978.5707\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1841893.1912 - mae: 1012.2475 - val_loss: 1450517.4194 - val_mae: 860.5732\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1799540.7383 - mae: 995.9139 - val_loss: 1686809.0612 - val_mae: 858.6841\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1805889.1129 - mae: 995.6085 - val_loss: 1588569.9474 - val_mae: 1036.3755\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1807450.0810 - mae: 999.7808 - val_loss: 1978774.0914 - val_mae: 1165.7338\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1817286.0836 - mae: 990.4410 - val_loss: 1526592.5165 - val_mae: 795.2237\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1748668.6007 - mae: 980.0114 - val_loss: 1313252.0939 - val_mae: 868.2131\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1720288.0438 - mae: 969.7137 - val_loss: 1581570.5510 - val_mae: 1033.1465\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1720377.3885 - mae: 960.7180 - val_loss: 1281191.5485 - val_mae: 851.2401\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1681377.1244 - mae: 955.2700 - val_loss: 1291615.8971 - val_mae: 838.5491\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1730847.5319 - mae: 969.0475 - val_loss: 1801499.5980 - val_mae: 851.0104\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1706035.7526 - mae: 962.8718 - val_loss: 1447922.7090 - val_mae: 975.1717\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1674500.4165 - mae: 949.4648 - val_loss: 2111728.7151 - val_mae: 954.7744\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1697073.3820 - mae: 951.1152 - val_loss: 1326973.6361 - val_mae: 916.6623\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1667017.2097 - mae: 949.0930 - val_loss: 2165359.0697 - val_mae: 985.4023\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1663592.9765 - mae: 943.8670 - val_loss: 1266017.3690 - val_mae: 749.7186\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1675087.4479 - mae: 941.1087 - val_loss: 2326211.1627 - val_mae: 1279.6387\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1690808.5542 - mae: 950.7244 - val_loss: 3327331.7469 - val_mae: 1531.6936\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1689751.5379 - mae: 947.0162 - val_loss: 2864200.1875 - val_mae: 1228.8778\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1662258.1861 - mae: 944.8931 - val_loss: 2174658.5810 - val_mae: 1225.4191\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 1s 137us/step - loss: 1683382.7973 - mae: 951.2421 - val_loss: 1201315.7582 - val_mae: 828.4274\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 1s 80us/step - loss: 1640558.1866 - mae: 934.7005 - val_loss: 1257843.8764 - val_mae: 851.5665\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 1s 132us/step - loss: 1689367.3487 - mae: 952.5222 - val_loss: 1303120.2718 - val_mae: 860.9744\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 1s 72us/step - loss: 1647698.5692 - mae: 941.0466 - val_loss: 1880331.1171 - val_mae: 850.7811\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1671966.2070 - mae: 945.9735 - val_loss: 1581199.7406 - val_mae: 755.0527\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1689833.5015 - mae: 949.8362 - val_loss: 1359950.1946 - val_mae: 934.6988\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1693523.4103 - mae: 946.7212 - val_loss: 1406487.4142 - val_mae: 747.2570\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1651355.7785 - mae: 940.6106 - val_loss: 2347980.4262 - val_mae: 1307.6477\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1614094.1289 - mae: 928.4083 - val_loss: 1515479.1022 - val_mae: 995.8484\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1644980.9328 - mae: 936.3662 - val_loss: 1848323.2002 - val_mae: 847.4327\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1701325.2663 - mae: 948.8467 - val_loss: 1875514.2461 - val_mae: 879.1156\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1665783.9339 - mae: 939.6318 - val_loss: 1468737.3445 - val_mae: 988.6777\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1662674.5205 - mae: 940.8800 - val_loss: 2788131.8339 - val_mae: 1428.7028\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1655597.6390 - mae: 935.7045 - val_loss: 1302228.3957 - val_mae: 761.6846\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1621757.6270 - mae: 930.0427 - val_loss: 1257469.2446 - val_mae: 706.2349\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1630507.0398 - mae: 932.9588 - val_loss: 1307355.4899 - val_mae: 875.1281\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1608344.8318 - mae: 926.2740 - val_loss: 1242812.9725 - val_mae: 880.3497\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1643297.3689 - mae: 932.1494 - val_loss: 1602707.7701 - val_mae: 755.1213\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1575952.8121 - mae: 908.4244 - val_loss: 1243912.3312 - val_mae: 753.8184\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1656049.0477 - mae: 937.8612 - val_loss: 1381645.1591 - val_mae: 703.9702\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1611406.5592 - mae: 922.2281 - val_loss: 1823203.7888 - val_mae: 818.2719\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1602082.5091 - mae: 922.1268 - val_loss: 1256262.3967 - val_mae: 841.2542\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1628676.6176 - mae: 933.0740 - val_loss: 1373289.1296 - val_mae: 934.1499\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1626158.6621 - mae: 924.4577 - val_loss: 2066293.2528 - val_mae: 1201.9603\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1583499.2607 - mae: 913.7909 - val_loss: 1807695.2008 - val_mae: 819.3802\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1621112.1125 - mae: 928.5211 - val_loss: 1261735.8159 - val_mae: 704.7968\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1609120.9014 - mae: 921.8442 - val_loss: 1494033.2131 - val_mae: 987.8553\n",
      "Epoch 103/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 56us/step - loss: 1625430.2640 - mae: 926.2755 - val_loss: 1898255.9004 - val_mae: 1133.0996\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1612657.6756 - mae: 932.3929 - val_loss: 1317567.3379 - val_mae: 902.0968\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1601324.6548 - mae: 925.2839 - val_loss: 1201418.6006 - val_mae: 721.9764\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1582531.2710 - mae: 915.5234 - val_loss: 1435715.0099 - val_mae: 961.1516\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1605022.2906 - mae: 923.9409 - val_loss: 2192554.4466 - val_mae: 1249.9479\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1658121.4106 - mae: 940.3862 - val_loss: 1463922.2991 - val_mae: 748.2288\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1600596.7362 - mae: 915.6088 - val_loss: 1202715.4110 - val_mae: 801.9185\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1628099.3815 - mae: 924.8334 - val_loss: 1559758.2007 - val_mae: 745.5142\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 1s 79us/step - loss: 1593677.3464 - mae: 921.5550 - val_loss: 1733557.4353 - val_mae: 781.1039\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1623884.6875 - mae: 921.5151 - val_loss: 1144760.4735 - val_mae: 715.2078\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 1s 113us/step - loss: 1617848.4867 - mae: 926.4396 - val_loss: 2179576.3956 - val_mae: 993.8687\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 1s 93us/step - loss: 1580503.8506 - mae: 915.6401 - val_loss: 1311389.6830 - val_mae: 881.3112\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1585343.4373 - mae: 913.4702 - val_loss: 1413578.4213 - val_mae: 979.8855\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1599475.6582 - mae: 916.5276 - val_loss: 1533986.2701 - val_mae: 738.1746\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 1577694.0675 - mae: 917.7869 - val_loss: 1266501.0102 - val_mae: 666.8688\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1543178.4926 - mae: 903.8359 - val_loss: 1257659.1138 - val_mae: 709.8568\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1585619.9810 - mae: 914.0118 - val_loss: 1182911.6842 - val_mae: 847.4680\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1574010.2316 - mae: 910.5844 - val_loss: 1755791.3280 - val_mae: 832.0074\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1605634.1875 - mae: 910.8199 - val_loss: 1422724.7811 - val_mae: 702.2737\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1605973.2625 - mae: 914.8817 - val_loss: 1237835.5504 - val_mae: 706.2955\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1572099.4439 - mae: 910.7330 - val_loss: 1696506.1818 - val_mae: 803.0849\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1574152.9076 - mae: 904.1991 - val_loss: 2142564.0323 - val_mae: 974.6689\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1539058.1574 - mae: 900.5632 - val_loss: 1948678.4370 - val_mae: 878.6754\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1562698.6067 - mae: 907.2694 - val_loss: 2213352.8211 - val_mae: 1000.3323\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1587040.6853 - mae: 911.1781 - val_loss: 1902010.9746 - val_mae: 1147.3180\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1567208.2427 - mae: 911.1826 - val_loss: 1863748.1043 - val_mae: 838.6951\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1541550.5883 - mae: 895.1735 - val_loss: 1226403.1961 - val_mae: 749.0949\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1566212.5556 - mae: 909.7277 - val_loss: 1651749.3384 - val_mae: 790.9951\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1598294.0280 - mae: 923.1525 - val_loss: 2135865.9595 - val_mae: 1257.0739\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1528172.7783 - mae: 901.5614 - val_loss: 1217728.7125 - val_mae: 664.8856\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1555210.8664 - mae: 902.3089 - val_loss: 1347975.4964 - val_mae: 671.7624\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1563666.1162 - mae: 903.3672 - val_loss: 1192095.3152 - val_mae: 837.5463\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1529812.2789 - mae: 889.8703 - val_loss: 2683800.0339 - val_mae: 1388.3921\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1538140.6362 - mae: 899.9517 - val_loss: 1752427.2863 - val_mae: 808.2391\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1565333.5040 - mae: 908.7152 - val_loss: 1108679.8287 - val_mae: 694.5745\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 1s 106us/step - loss: 1548602.5545 - mae: 904.3292 - val_loss: 2592415.1938 - val_mae: 1129.8848 - loss: 1503828.9292 - mae: \n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1529079.1133 - mae: 898.4558 - val_loss: 1147137.8413 - val_mae: 735.6858\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1569692.0022 - mae: 910.8003 - val_loss: 1164831.9231 - val_mae: 791.9374\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1535808.6031 - mae: 899.4578 - val_loss: 1121884.0119 - val_mae: 779.6163\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1535837.5262 - mae: 896.9099 - val_loss: 1278789.9996 - val_mae: 684.1437\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1536362.8287 - mae: 891.9351 - val_loss: 1377879.9902 - val_mae: 676.2144\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1545433.0736 - mae: 899.2552 - val_loss: 1104045.8459 - val_mae: 747.4488\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1536806.6298 - mae: 901.9860 - val_loss: 1742047.9117 - val_mae: 814.7111\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1509532.8137 - mae: 888.6085 - val_loss: 2540522.8302 - val_mae: 1073.0352\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1537701.6877 - mae: 893.8401 - val_loss: 1312909.1742 - val_mae: 709.1386\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1534248.5037 - mae: 893.8698 - val_loss: 1195185.3954 - val_mae: 845.1794\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1550762.9369 - mae: 903.4751 - val_loss: 1110848.0793 - val_mae: 768.9576\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1499424.1671 - mae: 886.3290 - val_loss: 1074766.0918 - val_mae: 773.2593\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1511796.1404 - mae: 893.5504 - val_loss: 1357180.2123 - val_mae: 963.1581\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1514102.4468 - mae: 893.6077 - val_loss: 1441579.5675 - val_mae: 693.2478\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1513938.7818 - mae: 887.0098 - val_loss: 1241814.7841 - val_mae: 915.5266\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1504954.0579 - mae: 881.9999 - val_loss: 1165404.4529 - val_mae: 701.5420\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1476860.3494 - mae: 873.6624 - val_loss: 2978999.2026 - val_mae: 1472.1641\n",
      "Epoch 156/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 58us/step - loss: 1490439.6901 - mae: 882.3965 - val_loss: 1241256.4602 - val_mae: 730.7563\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1515031.3617 - mae: 888.0961 - val_loss: 1187948.9225 - val_mae: 670.3973\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1523014.0168 - mae: 879.5014 - val_loss: 1160698.9747 - val_mae: 841.0566\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1453246.5296 - mae: 868.4573 - val_loss: 1234435.2353 - val_mae: 876.5231\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1517553.6583 - mae: 890.0984 - val_loss: 1222255.0675 - val_mae: 688.2396\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 79us/step - loss: 5108211.4438 - mae: 1838.3737 - val_loss: 4080491.6376 - val_mae: 1636.9280\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 4634141.3566 - mae: 1728.7028 - val_loss: 4161780.6966 - val_mae: 1645.3995\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 4494649.6648 - mae: 1702.0677 - val_loss: 3824501.7996 - val_mae: 1580.7126\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 4341851.4953 - mae: 1663.4636 - val_loss: 3747414.5682 - val_mae: 1555.5360\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 4227300.0474 - mae: 1634.1439 - val_loss: 3658332.0832 - val_mae: 1524.4829\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 4010127.8790 - mae: 1588.0459 - val_loss: 3380395.3741 - val_mae: 1476.1837\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3817608.3809 - mae: 1556.6830 - val_loss: 3300629.2921 - val_mae: 1471.2032\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3708285.1260 - mae: 1542.7644 - val_loss: 3253715.6253 - val_mae: 1459.4819\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3623968.9662 - mae: 1523.8207 - val_loss: 3222135.5738 - val_mae: 1464.9408\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 3679736.2482 - mae: 1540.1882 - val_loss: 3275513.4984 - val_mae: 1467.0208\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3624370.3211 - mae: 1526.2582 - val_loss: 3133243.1828 - val_mae: 1445.4573\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3548519.6921 - mae: 1511.5264 - val_loss: 3072847.4830 - val_mae: 1463.2115\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3532202.4079 - mae: 1510.5088 - val_loss: 3128602.8948 - val_mae: 1440.4269\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3534946.9046 - mae: 1510.6322 - val_loss: 3141058.0236 - val_mae: 1419.5110\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3481922.7429 - mae: 1498.0879 - val_loss: 3109902.6165 - val_mae: 1428.9763\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3450451.1247 - mae: 1481.8496 - val_loss: 2911994.3034 - val_mae: 1417.8903\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3376654.5738 - mae: 1467.8429 - val_loss: 2864543.5854 - val_mae: 1406.9955\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3326710.0965 - mae: 1457.3474 - val_loss: 2844070.3417 - val_mae: 1361.8125\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3180244.1184 - mae: 1424.6384 - val_loss: 2629875.8122 - val_mae: 1326.8931\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3089332.1713 - mae: 1397.5276 - val_loss: 2554426.2177 - val_mae: 1306.8936\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2899320.3386 - mae: 1341.2255 - val_loss: 2366343.6614 - val_mae: 1217.4800\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2829025.1143 - mae: 1314.5646 - val_loss: 2185144.8712 - val_mae: 1180.4056\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2692902.0050 - mae: 1274.7855 - val_loss: 2230953.0358 - val_mae: 1115.5820\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2566326.4023 - mae: 1236.4923 - val_loss: 2827318.0801 - val_mae: 1412.2466\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2572352.8294 - mae: 1232.1626 - val_loss: 2100978.2858 - val_mae: 1070.3115\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2515069.6388 - mae: 1219.7950 - val_loss: 2068463.5104 - val_mae: 1077.8214\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2476344.4303 - mae: 1206.2440 - val_loss: 1918913.0901 - val_mae: 1103.0540\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2448169.6528 - mae: 1196.1846 - val_loss: 2388432.0622 - val_mae: 1084.4802\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2421791.1996 - mae: 1185.9169 - val_loss: 1849295.3301 - val_mae: 1060.9337\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2444925.1326 - mae: 1195.5300 - val_loss: 2026389.6724 - val_mae: 1168.6030\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2374775.0307 - mae: 1176.5547 - val_loss: 1905291.9039 - val_mae: 1032.7045\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2345438.9270 - mae: 1171.3469 - val_loss: 1961895.3960 - val_mae: 1064.5731\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2387080.4152 - mae: 1185.8041 - val_loss: 1955382.1561 - val_mae: 1016.0712\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2351900.1201 - mae: 1168.5879 - val_loss: 1812392.4763 - val_mae: 1051.8639\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2309412.0662 - mae: 1161.0189 - val_loss: 2324859.0644 - val_mae: 1102.0625\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2286294.5611 - mae: 1158.7457 - val_loss: 1885037.0433 - val_mae: 1133.1283\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2270281.9432 - mae: 1154.2885 - val_loss: 1803057.0214 - val_mae: 1079.5989\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2284287.2694 - mae: 1152.3762 - val_loss: 1787544.2748 - val_mae: 998.2438\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2290176.7820 - mae: 1153.4762 - val_loss: 2375839.2855 - val_mae: 1073.8330\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2242926.7245 - mae: 1142.4237 - val_loss: 1717863.5532 - val_mae: 1002.5400\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2229468.1803 - mae: 1141.1230 - val_loss: 1663203.8425 - val_mae: 990.9969\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2241747.3467 - mae: 1136.9552 - val_loss: 1687912.0983 - val_mae: 1051.7271\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2179775.8032 - mae: 1121.6498 - val_loss: 2330716.2007 - val_mae: 1294.8566\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2189151.4087 - mae: 1119.4462 - val_loss: 1675408.5236 - val_mae: 1047.4572\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2160406.4645 - mae: 1119.1793 - val_loss: 1596612.9242 - val_mae: 953.3865\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2110938.1535 - mae: 1100.5410 - val_loss: 1659670.8759 - val_mae: 959.2272\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2128456.6548 - mae: 1106.3110 - val_loss: 1650201.7056 - val_mae: 1045.1232\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2099779.1647 - mae: 1096.8293 - val_loss: 1616172.6668 - val_mae: 1043.3910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2072899.3513 - mae: 1087.1294 - val_loss: 1466540.2351 - val_mae: 948.7228\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2043514.4892 - mae: 1075.8688 - val_loss: 1575721.9711 - val_mae: 892.2620\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2039561.4142 - mae: 1071.3589 - val_loss: 1718395.7933 - val_mae: 891.1024\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2004279.3183 - mae: 1060.7123 - val_loss: 1579919.1437 - val_mae: 896.5865\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1968037.1217 - mae: 1047.7904 - val_loss: 1504403.8660 - val_mae: 857.6251\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1966155.8082 - mae: 1047.3458 - val_loss: 1559063.4447 - val_mae: 851.5871\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1900929.6008 - mae: 1031.8403 - val_loss: 1327711.7249 - val_mae: 858.0088\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1910195.2435 - mae: 1032.3110 - val_loss: 1437919.0927 - val_mae: 987.9279\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1882780.4889 - mae: 1026.5282 - val_loss: 1652151.4907 - val_mae: 1060.4807\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1883502.7486 - mae: 1021.5368 - val_loss: 1417773.6792 - val_mae: 982.9598\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1821142.6553 - mae: 997.5723 - val_loss: 1504770.1910 - val_mae: 814.8027\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1825985.8585 - mae: 997.5585 - val_loss: 1263606.6399 - val_mae: 764.6366\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1787525.4662 - mae: 983.8771 - val_loss: 1282030.5258 - val_mae: 920.3682\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1800193.1725 - mae: 990.9960 - val_loss: 2014490.7186 - val_mae: 1190.6384\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1825344.0229 - mae: 994.7620 - val_loss: 1387987.4218 - val_mae: 970.5081\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1809421.5021 - mae: 985.2166 - val_loss: 1315585.2297 - val_mae: 950.0671\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1760479.4177 - mae: 971.6274 - val_loss: 1156268.5831 - val_mae: 790.8641\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1744168.5360 - mae: 968.8008 - val_loss: 1618984.5956 - val_mae: 835.8127\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1745001.3926 - mae: 963.6696 - val_loss: 1115047.6516 - val_mae: 828.8213\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1727578.4846 - mae: 965.8087 - val_loss: 1064681.0741 - val_mae: 804.1135\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1757490.3970 - mae: 970.6103 - val_loss: 1112682.1110 - val_mae: 790.8273\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1725964.7549 - mae: 963.5295 - val_loss: 1074919.7929 - val_mae: 747.0098\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1725547.0143 - mae: 961.6899 - val_loss: 1682618.5490 - val_mae: 852.0466\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1709404.0986 - mae: 958.6053 - val_loss: 1437697.2252 - val_mae: 994.8200\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1722842.9776 - mae: 966.1060 - val_loss: 1044161.0625 - val_mae: 754.8329\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1737838.0850 - mae: 960.8249 - val_loss: 1242537.4950 - val_mae: 909.2426\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1764907.1138 - mae: 965.6627 - val_loss: 1148547.1523 - val_mae: 716.8177\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1696343.0557 - mae: 954.5112 - val_loss: 1414099.8645 - val_mae: 974.6149\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1696379.4391 - mae: 951.9044 - val_loss: 1063476.8519 - val_mae: 739.8229\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1699562.6448 - mae: 955.2192 - val_loss: 2493782.4592 - val_mae: 1332.5676\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1718089.5907 - mae: 957.3026 - val_loss: 1408740.1836 - val_mae: 772.8289\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1730910.7811 - mae: 961.3949 - val_loss: 1187375.3745 - val_mae: 876.1345\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1694266.4154 - mae: 948.3006 - val_loss: 1055386.3602 - val_mae: 738.1984\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1726553.7136 - mae: 956.0514 - val_loss: 2508337.5471 - val_mae: 1174.9005\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1691256.7730 - mae: 950.7972 - val_loss: 1594325.7971 - val_mae: 1071.9867\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1687217.0657 - mae: 941.7325 - val_loss: 1206872.3774 - val_mae: 885.0457\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1676930.2204 - mae: 947.0579 - val_loss: 1118416.4772 - val_mae: 724.1281\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1685863.5887 - mae: 947.9991 - val_loss: 2125282.8849 - val_mae: 1048.4889\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1715061.0266 - mae: 953.3152 - val_loss: 1183028.4540 - val_mae: 891.8586\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1698985.8086 - mae: 951.4117 - val_loss: 1455909.8660 - val_mae: 753.0680\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1662146.1501 - mae: 942.5670 - val_loss: 1567790.3896 - val_mae: 1051.5153\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1688137.9488 - mae: 945.4383 - val_loss: 1464967.2230 - val_mae: 764.8844\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1657917.9937 - mae: 932.6433 - val_loss: 2114881.2579 - val_mae: 1242.8052\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1626964.5911 - mae: 931.7887 - val_loss: 1051681.9655 - val_mae: 675.0686\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1677444.1029 - mae: 944.1460 - val_loss: 1476856.7791 - val_mae: 1009.1597\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1661605.0760 - mae: 940.5705 - val_loss: 976728.3659 - val_mae: 727.3503\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1688600.7616 - mae: 943.2208 - val_loss: 1078629.1209 - val_mae: 777.6384\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1624712.2269 - mae: 929.2448 - val_loss: 1255720.4353 - val_mae: 717.5399\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1699188.3695 - mae: 948.2723 - val_loss: 1407534.6288 - val_mae: 1001.1203\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1673105.7676 - mae: 947.8004 - val_loss: 1023600.6750 - val_mae: 709.0649\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1649497.9454 - mae: 938.2418 - val_loss: 1004453.5319 - val_mae: 715.3832\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1670576.3049 - mae: 943.2989 - val_loss: 1732376.9227 - val_mae: 898.8729\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1645391.6623 - mae: 933.1784 - val_loss: 1047263.1954 - val_mae: 691.5231\n",
      "Epoch 102/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 57us/step - loss: 1665488.0041 - mae: 938.4913 - val_loss: 1032601.3052 - val_mae: 769.6602\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1648636.0810 - mae: 933.6101 - val_loss: 2673680.2032 - val_mae: 1415.1740\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1637911.5300 - mae: 934.5554 - val_loss: 1024784.3228 - val_mae: 682.8802\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1644245.7946 - mae: 925.6959 - val_loss: 1283059.0831 - val_mae: 724.7434\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1633259.7635 - mae: 931.9089 - val_loss: 1020275.7402 - val_mae: 785.2798\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1613152.4628 - mae: 924.4324 - val_loss: 1051161.1519 - val_mae: 679.8267\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 1s 106us/step - loss: 1655985.2507 - mae: 933.5225 - val_loss: 1703179.0647 - val_mae: 1084.5251\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1614141.8459 - mae: 928.9143 - val_loss: 2019217.4282 - val_mae: 1016.0868\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1639199.2181 - mae: 924.4474 - val_loss: 957517.7058 - val_mae: 737.2388\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1628333.0148 - mae: 927.5278 - val_loss: 1458464.6731 - val_mae: 783.0515\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1585989.3827 - mae: 912.8431 - val_loss: 1555746.4439 - val_mae: 798.7464\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1594864.9703 - mae: 920.1069 - val_loss: 1001690.6861 - val_mae: 745.6726\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1657302.5644 - mae: 939.3425 - val_loss: 3017781.7880 - val_mae: 1312.8494\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 1663726.2299 - mae: 926.8153 - val_loss: 1022371.3269 - val_mae: 758.6615\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1580832.0411 - mae: 915.4293 - val_loss: 1920543.6354 - val_mae: 960.2834\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1644899.1272 - mae: 927.8165 - val_loss: 1013171.9704 - val_mae: 788.7307\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1624760.4427 - mae: 928.2422 - val_loss: 1171694.9432 - val_mae: 685.4999\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1643827.4980 - mae: 928.0939 - val_loss: 1225983.6475 - val_mae: 696.5815\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1621426.6225 - mae: 924.6168 - val_loss: 1257693.0956 - val_mae: 932.7976\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1580735.0492 - mae: 909.5028 - val_loss: 2201670.6385 - val_mae: 1061.6559\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1585405.1334 - mae: 914.6288 - val_loss: 984142.8970 - val_mae: 689.0200\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1637873.5210 - mae: 922.0207 - val_loss: 1382582.1685 - val_mae: 758.8700\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1622760.9287 - mae: 918.8912 - val_loss: 1283699.9655 - val_mae: 714.1522\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1598417.6527 - mae: 919.3313 - val_loss: 1075864.5919 - val_mae: 840.8153\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1594603.2501 - mae: 914.5763 - val_loss: 1025989.7063 - val_mae: 795.1878\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1600643.5964 - mae: 919.0553 - val_loss: 962823.3365 - val_mae: 712.2686\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1603308.2118 - mae: 916.3690 - val_loss: 1005590.8869 - val_mae: 665.9606\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1614459.8152 - mae: 918.7690 - val_loss: 1010877.5445 - val_mae: 696.5561\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1641727.5946 - mae: 930.8010 - val_loss: 966904.8543 - val_mae: 705.4327\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1621220.7552 - mae: 917.5714 - val_loss: 1019048.9974 - val_mae: 644.2095\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1598535.5388 - mae: 913.3241 - val_loss: 926661.7074 - val_mae: 672.7281\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1604135.4346 - mae: 918.7861 - val_loss: 1104100.2916 - val_mae: 831.8942\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1592669.8954 - mae: 911.9672 - val_loss: 1203895.1533 - val_mae: 695.5274\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1578487.0914 - mae: 910.5320 - val_loss: 967549.6654 - val_mae: 767.9047\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1555197.1448 - mae: 900.1297 - val_loss: 1025762.3915 - val_mae: 687.1074\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1551865.1970 - mae: 899.8253 - val_loss: 1075423.5378 - val_mae: 727.4043\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1583007.1950 - mae: 914.2097 - val_loss: 2010510.8890 - val_mae: 989.8036\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1580980.6596 - mae: 908.6901 - val_loss: 927101.8802 - val_mae: 704.3114\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1554499.9304 - mae: 903.3804 - val_loss: 1302436.0870 - val_mae: 713.5816\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1533670.0017 - mae: 894.2882 - val_loss: 1229703.3901 - val_mae: 929.9270\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1592357.5884 - mae: 911.3802 - val_loss: 1254066.3772 - val_mae: 920.6349\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1557777.7982 - mae: 905.7927 - val_loss: 1027215.5134 - val_mae: 665.0333\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1575940.7814 - mae: 905.7984 - val_loss: 1268894.5851 - val_mae: 721.0770\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1571645.2551 - mae: 906.7238 - val_loss: 1424347.8218 - val_mae: 1020.6832\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1560809.9494 - mae: 901.7830 - val_loss: 1116823.3539 - val_mae: 847.0826\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1595393.1353 - mae: 916.2305 - val_loss: 985664.3768 - val_mae: 778.6028\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1598739.6164 - mae: 912.0320 - val_loss: 1261633.1628 - val_mae: 694.3309\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 1s 75us/step - loss: 1594520.7602 - mae: 910.6227 - val_loss: 1147148.3161 - val_mae: 650.6158\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1584356.2406 - mae: 907.5124 - val_loss: 1515705.6084 - val_mae: 1034.4603\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1588181.6417 - mae: 904.6624 - val_loss: 2142713.0616 - val_mae: 1244.8613\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1523169.8714 - mae: 893.5554 - val_loss: 1003387.2687 - val_mae: 669.1124\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1508181.5108 - mae: 881.4083 - val_loss: 913670.3898 - val_mae: 717.7089\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1529139.7701 - mae: 895.6816 - val_loss: 1083865.9734 - val_mae: 642.3453\n",
      "Epoch 155/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 74us/step - loss: 1525799.7442 - mae: 885.7219 - val_loss: 942449.5464 - val_mae: 666.2200\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1541034.0138 - mae: 895.2227 - val_loss: 1896324.8626 - val_mae: 951.3071\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 1s 73us/step - loss: 1545186.6251 - mae: 896.9286 - val_loss: 1520461.5430 - val_mae: 796.9238\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1560428.0332 - mae: 904.4633 - val_loss: 1249799.7907 - val_mae: 943.9226\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1522475.0340 - mae: 889.6420 - val_loss: 2219809.6420 - val_mae: 1277.5867\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1575739.6298 - mae: 904.4238 - val_loss: 1836539.2252 - val_mae: 1162.4055\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 95us/step - loss: 5325646.7129 - mae: 1885.7249 - val_loss: 4636472.7117 - val_mae: 1727.8855\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 4647733.5946 - mae: 1738.2800 - val_loss: 4582846.4378 - val_mae: 1710.0338\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 4370451.1305 - mae: 1678.2562 - val_loss: 4436377.6790 - val_mae: 1673.1130\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 4206248.3889 - mae: 1636.9478 - val_loss: 4142138.9962 - val_mae: 1603.4169\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3949494.2698 - mae: 1575.2688 - val_loss: 3726000.0534 - val_mae: 1548.2657\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 3797588.6214 - mae: 1550.3920 - val_loss: 3649631.3788 - val_mae: 1524.8480\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 3699623.4562 - mae: 1530.7496 - val_loss: 3578659.2535 - val_mae: 1522.5911\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 3661766.7677 - mae: 1528.0931 - val_loss: 3486087.7984 - val_mae: 1519.0679\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 3656043.8512 - mae: 1531.3617 - val_loss: 3462633.1828 - val_mae: 1515.2748\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 3639246.7964 - mae: 1524.2354 - val_loss: 3483408.0003 - val_mae: 1504.7017\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3616871.4025 - mae: 1523.2944 - val_loss: 3427941.3615 - val_mae: 1517.8892\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3559817.2165 - mae: 1510.8538 - val_loss: 3368868.0408 - val_mae: 1508.0428\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 3503060.2686 - mae: 1495.7706 - val_loss: 3353628.6649 - val_mae: 1523.6312\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 3472933.6027 - mae: 1491.8671 - val_loss: 3406695.7126 - val_mae: 1475.7268\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 3411819.7232 - mae: 1477.1736 - val_loss: 3390967.1680 - val_mae: 1461.1838\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 1s 75us/step - loss: 3422166.7453 - mae: 1478.3416 - val_loss: 3214155.6624 - val_mae: 1471.5215\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3338891.9114 - mae: 1460.9100 - val_loss: 3211822.1492 - val_mae: 1426.7715\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3289743.7366 - mae: 1443.4088 - val_loss: 3210095.1875 - val_mae: 1403.9381\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3171307.2103 - mae: 1418.2233 - val_loss: 2860525.5210 - val_mae: 1363.1113\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3033298.0679 - mae: 1381.6733 - val_loss: 2893122.1316 - val_mae: 1389.2731\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2914056.1694 - mae: 1334.4381 - val_loss: 2604591.3078 - val_mae: 1305.6855\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2752348.2191 - mae: 1289.5861 - val_loss: 2374829.9413 - val_mae: 1211.3793\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2643783.9396 - mae: 1262.8333 - val_loss: 2259756.1171 - val_mae: 1154.1299\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2568150.7964 - mae: 1234.5620 - val_loss: 2425024.2264 - val_mae: 1277.5248\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2493267.8149 - mae: 1211.6215 - val_loss: 3763966.9931 - val_mae: 1606.6392\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 2468617.0376 - mae: 1204.5002 - val_loss: 2730210.3106 - val_mae: 1176.5034\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2479089.1714 - mae: 1208.0703 - val_loss: 2333599.3876 - val_mae: 1259.1700\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2426042.2631 - mae: 1194.6317 - val_loss: 2072656.7456 - val_mae: 1099.8051\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2441358.6567 - mae: 1198.5787 - val_loss: 2083155.5537 - val_mae: 1093.9873\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2422953.9501 - mae: 1191.2374 - val_loss: 2014859.7271 - val_mae: 1113.3815\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2396834.5896 - mae: 1186.0519 - val_loss: 2060424.2572 - val_mae: 1080.1122\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2378360.1024 - mae: 1179.0781 - val_loss: 2241107.9042 - val_mae: 1068.1705\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2385795.7045 - mae: 1179.3765 - val_loss: 2174647.2465 - val_mae: 1221.5753\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2345695.3397 - mae: 1170.3468 - val_loss: 3184035.4893 - val_mae: 1502.0834\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2302373.1057 - mae: 1157.4021 - val_loss: 1954792.2484 - val_mae: 1096.8353\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2276715.9472 - mae: 1153.0404 - val_loss: 2125432.3428 - val_mae: 1052.6139\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2281505.6885 - mae: 1149.7756 - val_loss: 2640963.7758 - val_mae: 1363.7617\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2315365.4546 - mae: 1165.3561 - val_loss: 1879140.1124 - val_mae: 1026.0685\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 1s 82us/step - loss: 2217921.0425 - mae: 1140.5839 - val_loss: 2790325.8634 - val_mae: 1396.6086\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2250832.8762 - mae: 1143.4867 - val_loss: 1903622.1407 - val_mae: 1088.4524\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2220358.8310 - mae: 1133.6755 - val_loss: 1897350.6859 - val_mae: 1008.1122\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2185213.3604 - mae: 1124.3184 - val_loss: 1868741.4155 - val_mae: 993.4598\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2204611.0838 - mae: 1129.3424 - val_loss: 2158586.4768 - val_mae: 1204.7963\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2183378.6344 - mae: 1121.7401 - val_loss: 1756795.9081 - val_mae: 1021.1025\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2133309.6331 - mae: 1114.9883 - val_loss: 2313561.5537 - val_mae: 1059.0652\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2113027.7479 - mae: 1105.6378 - val_loss: 1713230.5484 - val_mae: 1001.2170\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2151030.2368 - mae: 1107.8041 - val_loss: 2077824.7352 - val_mae: 1008.5910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2118160.1682 - mae: 1099.0796 - val_loss: 1817774.3065 - val_mae: 969.1333\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2057258.3504 - mae: 1080.8014 - val_loss: 2127757.0751 - val_mae: 1213.8069\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2017224.5506 - mae: 1075.1334 - val_loss: 1644662.7943 - val_mae: 912.0546\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2064649.9325 - mae: 1090.4124 - val_loss: 1764098.3480 - val_mae: 944.9753\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2010363.8500 - mae: 1069.6337 - val_loss: 4680745.5201 - val_mae: 1809.4496\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1998443.0252 - mae: 1066.3563 - val_loss: 1525355.6908 - val_mae: 950.4115\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1972502.0007 - mae: 1055.0492 - val_loss: 2071964.9180 - val_mae: 982.8506\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1927754.5034 - mae: 1038.7587 - val_loss: 1519000.4201 - val_mae: 964.4244\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1943689.6698 - mae: 1040.8723 - val_loss: 1425803.9587 - val_mae: 887.0827\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1924763.1051 - mae: 1035.5487 - val_loss: 2339291.3094 - val_mae: 1289.0261\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1881180.4879 - mae: 1023.4252 - val_loss: 2232529.0652 - val_mae: 1042.6990\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1877438.8597 - mae: 1018.9924 - val_loss: 1417032.2448 - val_mae: 928.2105\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1843483.4825 - mae: 1002.4094 - val_loss: 1461850.2257 - val_mae: 817.0219\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1811700.2661 - mae: 995.3719 - val_loss: 1402696.3662 - val_mae: 938.7234\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1830969.6285 - mae: 1000.8217 - val_loss: 1662764.1107 - val_mae: 850.7226\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1807257.6706 - mae: 987.7531 - val_loss: 1264996.9102 - val_mae: 848.9643\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1755449.9536 - mae: 978.3052 - val_loss: 1297651.0449 - val_mae: 893.4104\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1726693.5944 - mae: 968.5223 - val_loss: 1347489.8323 - val_mae: 785.5633\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 1s 98us/step - loss: 1759854.4401 - mae: 973.0398 - val_loss: 1562387.3943 - val_mae: 1047.1655\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1809317.0148 - mae: 986.4128 - val_loss: 3528500.1822 - val_mae: 1595.1385\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1730377.2679 - mae: 963.6908 - val_loss: 1200420.9552 - val_mae: 795.7476\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1744962.4660 - mae: 968.2151 - val_loss: 1485725.4863 - val_mae: 1005.5493\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1712542.0117 - mae: 962.1655 - val_loss: 1507590.8117 - val_mae: 1002.5154\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1727657.7142 - mae: 969.4521 - val_loss: 1410028.5814 - val_mae: 794.0888\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1744891.1576 - mae: 971.2336 - val_loss: 1467829.8975 - val_mae: 965.8126\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1700475.0527 - mae: 954.8218 - val_loss: 1284003.6563 - val_mae: 745.6889\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1727317.0898 - mae: 953.0063 - val_loss: 1340659.0671 - val_mae: 727.2985\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1708546.7799 - mae: 958.8095 - val_loss: 2257802.6035 - val_mae: 1076.6410\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1662245.0371 - mae: 937.4513 - val_loss: 1858942.6776 - val_mae: 892.7753\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1683328.4140 - mae: 948.4180 - val_loss: 1200116.6873 - val_mae: 733.7223\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1691546.1226 - mae: 947.9001 - val_loss: 2079574.9551 - val_mae: 1002.8785\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1694342.0108 - mae: 948.1138 - val_loss: 1344592.6303 - val_mae: 724.8981\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1684368.9244 - mae: 950.7696 - val_loss: 1216382.2809 - val_mae: 723.4759\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1695375.5172 - mae: 948.3812 - val_loss: 1158990.0137 - val_mae: 809.6213\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1627818.4423 - mae: 932.0107 - val_loss: 1330918.2423 - val_mae: 952.1568\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1686559.6708 - mae: 956.1290 - val_loss: 1689458.1380 - val_mae: 826.6697\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1676593.2617 - mae: 947.4701 - val_loss: 1773886.1790 - val_mae: 860.6516\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1642396.4927 - mae: 931.9827 - val_loss: 1284988.1981 - val_mae: 736.8766\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1691835.6735 - mae: 944.8724 - val_loss: 1578725.4579 - val_mae: 1051.7972\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1664918.5942 - mae: 945.5485 - val_loss: 2492017.0157 - val_mae: 1355.5131\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1676667.7198 - mae: 941.9393 - val_loss: 1405970.6504 - val_mae: 1000.8593\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1664414.3358 - mae: 938.3058 - val_loss: 1795489.7767 - val_mae: 879.1546\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1703647.0094 - mae: 950.2104 - val_loss: 1333482.1842 - val_mae: 955.6270\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1628011.0618 - mae: 933.7051 - val_loss: 1288534.0632 - val_mae: 771.7486\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1655056.2635 - mae: 936.8297 - val_loss: 1412017.9004 - val_mae: 745.2448\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1627839.1109 - mae: 937.4366 - val_loss: 1302447.6505 - val_mae: 752.5790\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1667480.7846 - mae: 943.7029 - val_loss: 1362321.5621 - val_mae: 723.4343\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1616493.3376 - mae: 917.4116 - val_loss: 1480527.0714 - val_mae: 755.8848\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1676369.7990 - mae: 941.7557 - val_loss: 1800270.1693 - val_mae: 1130.1646\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1663690.3876 - mae: 933.6592 - val_loss: 1162195.0201 - val_mae: 720.0489\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1639492.9087 - mae: 929.7216 - val_loss: 1258791.9158 - val_mae: 932.1508\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1676853.2454 - mae: 946.1122 - val_loss: 1111833.6690 - val_mae: 718.1485\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1650886.9151 - mae: 932.1683 - val_loss: 1341756.4515 - val_mae: 744.8533\n",
      "Epoch 101/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 72us/step - loss: 1653160.3629 - mae: 931.8078 - val_loss: 1152795.1458 - val_mae: 800.6050\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1621440.5727 - mae: 932.4337 - val_loss: 1471984.7849 - val_mae: 747.0853\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1659390.7748 - mae: 935.5407 - val_loss: 1231493.9807 - val_mae: 878.6404\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1619895.3710 - mae: 920.9311 - val_loss: 1231386.4323 - val_mae: 687.8290\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1593163.0470 - mae: 922.6765 - val_loss: 1226271.2806 - val_mae: 908.9899\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1634230.7127 - mae: 922.8000 - val_loss: 1459392.9970 - val_mae: 998.7476\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1639900.4523 - mae: 929.6461 - val_loss: 1496746.4994 - val_mae: 994.3425\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1630847.0282 - mae: 925.5964 - val_loss: 1151946.7251 - val_mae: 745.9056\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1628711.9388 - mae: 933.7690 - val_loss: 1392113.0673 - val_mae: 730.6410\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1638225.9189 - mae: 927.7660 - val_loss: 1787178.5713 - val_mae: 841.9442\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1621243.0966 - mae: 925.5082 - val_loss: 1194547.2829 - val_mae: 763.9689\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1619790.1965 - mae: 930.6630 - val_loss: 1527320.6580 - val_mae: 1012.5998\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1611275.2034 - mae: 922.6771 - val_loss: 1083871.2639 - val_mae: 761.0060\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1581688.1992 - mae: 908.4412 - val_loss: 1155013.5198 - val_mae: 815.3685\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1620997.5249 - mae: 923.0638 - val_loss: 2148526.8252 - val_mae: 1017.0757\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1648550.9371 - mae: 931.8588 - val_loss: 1349897.8948 - val_mae: 978.4841\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1630743.2868 - mae: 921.8323 - val_loss: 1255334.2823 - val_mae: 682.7942\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1590648.9901 - mae: 910.5261 - val_loss: 2571963.6387 - val_mae: 1188.8928\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1634190.3467 - mae: 930.1797 - val_loss: 1517064.4736 - val_mae: 758.6445\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1628495.4266 - mae: 921.7673 - val_loss: 1332119.1029 - val_mae: 716.1141\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1615362.9298 - mae: 917.2054 - val_loss: 2055321.7941 - val_mae: 994.7352\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1636389.6192 - mae: 923.4147 - val_loss: 4211643.8185 - val_mae: 1758.6216\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1594758.6951 - mae: 914.6284 - val_loss: 1142578.1431 - val_mae: 763.9531\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1582819.8676 - mae: 915.4205 - val_loss: 2186880.4655 - val_mae: 1051.4127\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1618895.4133 - mae: 922.2645 - val_loss: 2130999.4468 - val_mae: 1037.9513\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1643405.4191 - mae: 923.9947 - val_loss: 2023955.4095 - val_mae: 1216.1674\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1550784.6527 - mae: 909.9187 - val_loss: 2908147.2943 - val_mae: 1457.6597\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1591557.3492 - mae: 913.8467 - val_loss: 2232594.5634 - val_mae: 1050.0681\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1597233.5307 - mae: 919.8758 - val_loss: 1121403.0329 - val_mae: 691.1812\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1577266.0089 - mae: 907.7767 - val_loss: 1551702.8279 - val_mae: 1053.9297\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1576312.7061 - mae: 907.1172 - val_loss: 2929850.4105 - val_mae: 1478.5419\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1585117.2862 - mae: 908.5620 - val_loss: 1542845.5192 - val_mae: 1064.8552\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1583621.4074 - mae: 908.2246 - val_loss: 1129482.3860 - val_mae: 682.9704\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1581487.0533 - mae: 907.7125 - val_loss: 1462220.0371 - val_mae: 996.2018\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1574712.7319 - mae: 913.7654 - val_loss: 1757779.9633 - val_mae: 879.2498\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1578529.3696 - mae: 911.4033 - val_loss: 2155936.0006 - val_mae: 1017.6592\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1610775.7364 - mae: 916.6270 - val_loss: 1191469.7734 - val_mae: 886.4210\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1570716.8145 - mae: 909.6369 - val_loss: 1330365.3099 - val_mae: 703.0784\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1568133.0346 - mae: 901.1049 - val_loss: 1331155.5645 - val_mae: 714.8870\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1591729.8299 - mae: 913.6028 - val_loss: 1417359.1270 - val_mae: 958.6299\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1583630.7141 - mae: 911.6062 - val_loss: 1166873.4271 - val_mae: 704.6454\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1580500.7783 - mae: 906.4273 - val_loss: 1472555.2019 - val_mae: 1025.6509\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1598458.6833 - mae: 913.9345 - val_loss: 1365440.0174 - val_mae: 716.3186\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1580051.5072 - mae: 905.6717 - val_loss: 1048415.0046 - val_mae: 719.1001\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1585641.5431 - mae: 910.2687 - val_loss: 1171537.4131 - val_mae: 721.7494\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1533869.3512 - mae: 898.4048 - val_loss: 1421417.6698 - val_mae: 734.8515\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1583122.3368 - mae: 913.3799 - val_loss: 1711662.2164 - val_mae: 1134.1718\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1544574.4319 - mae: 897.5269 - val_loss: 1120620.8569 - val_mae: 831.9945\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1544865.2352 - mae: 896.1255 - val_loss: 1204461.7550 - val_mae: 890.1609\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1565450.6217 - mae: 905.4296 - val_loss: 1342549.8948 - val_mae: 711.3632\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1560373.2680 - mae: 894.9578 - val_loss: 1197618.8157 - val_mae: 726.1566\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1591016.4999 - mae: 909.9858 - val_loss: 2182160.2420 - val_mae: 1012.1206\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1565667.3178 - mae: 896.9497 - val_loss: 2594647.2663 - val_mae: 1389.4796\n",
      "Epoch 154/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 56us/step - loss: 1560175.9991 - mae: 906.3623 - val_loss: 1120134.5930 - val_mae: 816.9702\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1539482.4099 - mae: 893.8967 - val_loss: 1093492.3974 - val_mae: 755.4645\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1532253.2030 - mae: 881.6177 - val_loss: 1960298.8075 - val_mae: 1189.8729\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1555099.8857 - mae: 897.3838 - val_loss: 1093902.9855 - val_mae: 703.7331\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1550601.4191 - mae: 893.7789 - val_loss: 1941378.5719 - val_mae: 962.5283\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1511368.0078 - mae: 888.3023 - val_loss: 1210394.0665 - val_mae: 714.5933\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1552631.4140 - mae: 894.9407 - val_loss: 1103178.4432 - val_mae: 687.4636\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 82us/step - loss: 4877835.1884 - mae: 1794.9331 - val_loss: 4356930.7443 - val_mae: 1670.9957\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 4566865.2166 - mae: 1719.9370 - val_loss: 4098627.9366 - val_mae: 1612.0892\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 4395701.6822 - mae: 1678.7336 - val_loss: 3925226.1206 - val_mae: 1567.6345\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 4105232.5000 - mae: 1605.9304 - val_loss: 3802398.2057 - val_mae: 1517.5132\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3879790.5233 - mae: 1561.2961 - val_loss: 3320270.9746 - val_mae: 1439.5970\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 3760353.0454 - mae: 1543.5399 - val_loss: 3788307.7937 - val_mae: 1500.4055\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3675075.0329 - mae: 1533.7994 - val_loss: 3292844.5320 - val_mae: 1456.0537\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3641998.6728 - mae: 1524.6924 - val_loss: 3347313.8116 - val_mae: 1469.1766\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3608725.0947 - mae: 1519.0287 - val_loss: 3093710.3904 - val_mae: 1433.3439\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 3548571.7659 - mae: 1504.1656 - val_loss: 3124092.2340 - val_mae: 1423.5909\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3496972.4479 - mae: 1486.2236 - val_loss: 3088415.5983 - val_mae: 1419.6909\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3483720.0536 - mae: 1495.0955 - val_loss: 3206123.4752 - val_mae: 1415.7239\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3438053.4065 - mae: 1481.6090 - val_loss: 2986872.1388 - val_mae: 1397.4242\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3331380.4995 - mae: 1457.8881 - val_loss: 2829010.0729 - val_mae: 1363.3601\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 1s 75us/step - loss: 3302778.7732 - mae: 1449.1329 - val_loss: 2782267.1269 - val_mae: 1363.2904\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 3220929.5258 - mae: 1433.9243 - val_loss: 2756516.7067 - val_mae: 1329.9105\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 3172579.6647 - mae: 1408.0016 - val_loss: 2629735.0415 - val_mae: 1291.8278\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3055514.9481 - mae: 1385.0713 - val_loss: 2496908.7214 - val_mae: 1271.7434\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2946687.1883 - mae: 1348.9447 - val_loss: 2402395.5958 - val_mae: 1225.7252\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2884349.8140 - mae: 1335.9592 - val_loss: 2491305.3439 - val_mae: 1210.6730\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2829997.0096 - mae: 1314.1641 - val_loss: 2526277.0119 - val_mae: 1227.2200\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2751187.4559 - mae: 1285.5194 - val_loss: 2265201.9306 - val_mae: 1153.2924\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2645152.0818 - mae: 1260.6353 - val_loss: 2125393.4742 - val_mae: 1173.1270\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2623863.4618 - mae: 1252.0277 - val_loss: 2144285.0074 - val_mae: 1202.6287\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2604013.9966 - mae: 1245.0425 - val_loss: 2562371.3970 - val_mae: 1169.3644\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2536197.9021 - mae: 1223.5499 - val_loss: 2442958.1482 - val_mae: 1176.9352\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2518466.7877 - mae: 1223.3419 - val_loss: 2018761.3244 - val_mae: 1148.3989\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2458606.0188 - mae: 1204.5082 - val_loss: 2294938.9997 - val_mae: 1134.4885\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2435265.1457 - mae: 1199.1740 - val_loss: 2022773.6635 - val_mae: 1161.9277\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2427846.4607 - mae: 1198.6084 - val_loss: 1853527.3791 - val_mae: 1088.4517\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2360407.7783 - mae: 1176.3846 - val_loss: 2225711.1123 - val_mae: 1096.6761\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2354438.0311 - mae: 1171.0728 - val_loss: 1994348.6068 - val_mae: 1033.1980\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2327383.8544 - mae: 1173.5020 - val_loss: 1838065.3464 - val_mae: 1100.3044\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2268102.3496 - mae: 1154.4468 - val_loss: 2230293.8920 - val_mae: 1054.3313\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2282588.1834 - mae: 1157.9141 - val_loss: 1754933.8951 - val_mae: 1009.7366\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2263361.9173 - mae: 1150.4065 - val_loss: 1944890.3197 - val_mae: 1156.7439\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2270426.3516 - mae: 1154.4968 - val_loss: 2655237.2613 - val_mae: 1379.9020\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2237480.1940 - mae: 1139.1969 - val_loss: 2484448.8188 - val_mae: 1328.1926\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2211469.8929 - mae: 1145.7758 - val_loss: 1739241.8266 - val_mae: 1013.9339\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2238917.3279 - mae: 1138.4318 - val_loss: 2218477.1358 - val_mae: 1252.4161\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2199217.1068 - mae: 1126.1110 - val_loss: 2227573.3530 - val_mae: 1262.9404\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2204925.4582 - mae: 1131.3883 - val_loss: 1860726.5883 - val_mae: 1133.5634\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2137934.4026 - mae: 1113.6781 - val_loss: 1653372.9375 - val_mae: 928.9537\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2111911.9063 - mae: 1108.0127 - val_loss: 1544679.2313 - val_mae: 957.2813\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2045809.1764 - mae: 1085.1753 - val_loss: 1770261.1687 - val_mae: 958.3152\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2083172.0417 - mae: 1095.7728 - val_loss: 2091917.3161 - val_mae: 1004.7946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2033704.3004 - mae: 1075.5253 - val_loss: 1563386.6600 - val_mae: 896.1063\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1992785.9849 - mae: 1066.3623 - val_loss: 1596566.2629 - val_mae: 904.0228\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1988568.3880 - mae: 1068.8586 - val_loss: 1491665.8211 - val_mae: 994.1143\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1938428.5376 - mae: 1051.3674 - val_loss: 1469827.2082 - val_mae: 913.9169\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1923231.8349 - mae: 1043.6578 - val_loss: 2092319.2574 - val_mae: 1007.1642\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1927811.4814 - mae: 1032.4180 - val_loss: 1909553.8282 - val_mae: 937.2004\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1847233.7040 - mae: 1012.3380 - val_loss: 1522210.1453 - val_mae: 991.6973\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1828476.0837 - mae: 1001.9601 - val_loss: 1730522.7303 - val_mae: 871.1505\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1823862.3482 - mae: 1004.0799 - val_loss: 2424879.7888 - val_mae: 1130.9762\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1808669.7048 - mae: 998.3751 - val_loss: 1510174.7795 - val_mae: 1010.4025\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1813899.5471 - mae: 989.7769 - val_loss: 2260526.0470 - val_mae: 1247.8829\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1745908.9486 - mae: 971.7009 - val_loss: 1162011.3633 - val_mae: 802.6481\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1730581.9228 - mae: 971.6334 - val_loss: 1198393.8288 - val_mae: 738.5894\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1763844.3789 - mae: 977.8732 - val_loss: 3322507.2440 - val_mae: 1531.7860\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1736731.7689 - mae: 969.6089 - val_loss: 1957714.6438 - val_mae: 1175.8379\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1750841.0234 - mae: 965.5592 - val_loss: 2268456.8915 - val_mae: 1064.5477\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1694904.3485 - mae: 959.9966 - val_loss: 1312730.6926 - val_mae: 728.1417\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1743222.6759 - mae: 963.1139 - val_loss: 2217219.8219 - val_mae: 1256.3722\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1765658.8096 - mae: 973.7064 - val_loss: 2249497.9647 - val_mae: 1062.3357\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1730260.9393 - mae: 964.0114 - val_loss: 1199887.3604 - val_mae: 768.6804\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1712888.2384 - mae: 959.6152 - val_loss: 1160759.2146 - val_mae: 754.8996\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1703423.6188 - mae: 952.7415 - val_loss: 1464964.6492 - val_mae: 759.4484\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1728665.4583 - mae: 964.7383 - val_loss: 1819383.0396 - val_mae: 1132.8384\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1693999.7661 - mae: 950.9727 - val_loss: 1261522.2339 - val_mae: 905.2051\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1706183.5174 - mae: 947.7285 - val_loss: 1146153.2912 - val_mae: 725.7333\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1642275.0456 - mae: 942.6894 - val_loss: 1233713.9714 - val_mae: 876.9276\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1707021.6404 - mae: 952.6717 - val_loss: 1422764.8295 - val_mae: 742.5244\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1703209.0237 - mae: 952.1796 - val_loss: 1167138.9045 - val_mae: 834.4239\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1667028.1253 - mae: 942.2943 - val_loss: 1198041.3056 - val_mae: 746.8918\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1697433.8886 - mae: 948.5535 - val_loss: 1274792.6029 - val_mae: 910.7889\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1681942.5457 - mae: 950.8514 - val_loss: 2523820.7491 - val_mae: 1185.3608\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1688787.0184 - mae: 947.0994 - val_loss: 1724828.3279 - val_mae: 867.1215\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1660919.0115 - mae: 943.0397 - val_loss: 2151028.5055 - val_mae: 1244.8403\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1694997.3765 - mae: 952.8296 - val_loss: 1159170.4080 - val_mae: 693.5568\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1718028.0833 - mae: 949.2393 - val_loss: 1091299.3340 - val_mae: 749.6886\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1744138.0654 - mae: 963.0494 - val_loss: 1043545.2125 - val_mae: 746.9829\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1703025.1641 - mae: 949.5311 - val_loss: 1203782.7582 - val_mae: 888.9011\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1651889.8064 - mae: 940.5351 - val_loss: 2376648.6856 - val_mae: 1298.2770\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1656817.6748 - mae: 928.2750 - val_loss: 2177362.4907 - val_mae: 1036.5269\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1649992.0631 - mae: 935.7245 - val_loss: 1115646.0587 - val_mae: 691.4863\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1662551.9953 - mae: 939.7045 - val_loss: 1477675.5130 - val_mae: 982.3387\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1635786.8798 - mae: 931.1407 - val_loss: 1370897.2725 - val_mae: 941.4352\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1692117.4010 - mae: 944.3377 - val_loss: 1091939.7825 - val_mae: 715.0596\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1676886.8696 - mae: 943.2119 - val_loss: 1772462.8702 - val_mae: 863.7385\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1645229.5455 - mae: 935.7459 - val_loss: 2252160.2467 - val_mae: 1032.1193\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1627555.0670 - mae: 925.4476 - val_loss: 1597745.6972 - val_mae: 789.3391\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1660186.7866 - mae: 932.0349 - val_loss: 3997827.5057 - val_mae: 1694.2660\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1699833.2896 - mae: 952.7701 - val_loss: 1131428.5857 - val_mae: 717.9344\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1624069.2734 - mae: 922.2355 - val_loss: 2256108.4144 - val_mae: 1254.9607\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1636310.7409 - mae: 932.4734 - val_loss: 1185580.7561 - val_mae: 678.3440\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1653044.8565 - mae: 932.3712 - val_loss: 1155245.3326 - val_mae: 798.3386\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1600958.7728 - mae: 921.1345 - val_loss: 1188515.3160 - val_mae: 747.4131\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1627813.7769 - mae: 929.3004 - val_loss: 1171249.8788 - val_mae: 683.2859\n",
      "Epoch 100/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 58us/step - loss: 1640321.9926 - mae: 934.2587 - val_loss: 1235852.8567 - val_mae: 875.4735\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1610529.8616 - mae: 922.9403 - val_loss: 2091919.1453 - val_mae: 1001.9717\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1631037.6286 - mae: 926.9882 - val_loss: 1410556.1096 - val_mae: 731.2610\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 0s 70us/step - loss: 1628537.0689 - mae: 930.3832 - val_loss: 1173684.5468 - val_mae: 724.3408\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 1626989.2375 - mae: 922.2109 - val_loss: 1254524.2994 - val_mae: 694.6149\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1630929.0550 - mae: 927.3237 - val_loss: 1892218.3728 - val_mae: 1164.4116\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1622357.1552 - mae: 924.6560 - val_loss: 1095258.3898 - val_mae: 757.4952\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1615319.7915 - mae: 924.4979 - val_loss: 1133415.9985 - val_mae: 840.7986\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1631211.6274 - mae: 928.9025 - val_loss: 1031587.0815 - val_mae: 740.9781\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1585157.3312 - mae: 909.5779 - val_loss: 1702620.0615 - val_mae: 863.7856\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1582827.4622 - mae: 911.1704 - val_loss: 1243813.1107 - val_mae: 688.1006\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1626644.9776 - mae: 921.0325 - val_loss: 1064490.0892 - val_mae: 774.7228\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1554489.8847 - mae: 901.6667 - val_loss: 1219082.3669 - val_mae: 869.5532\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1600345.4786 - mae: 918.9937 - val_loss: 3039200.9915 - val_mae: 1482.8450\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1608894.2919 - mae: 927.3196 - val_loss: 1504201.0455 - val_mae: 767.1705\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1599504.3954 - mae: 917.2332 - val_loss: 1854329.7888 - val_mae: 930.1219\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1571742.0673 - mae: 909.8420 - val_loss: 1300924.5187 - val_mae: 938.2883\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1617173.6393 - mae: 922.2487 - val_loss: 1093361.5291 - val_mae: 688.9305\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1616907.3974 - mae: 925.0523 - val_loss: 1191314.7620 - val_mae: 700.0848\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1603509.1286 - mae: 918.1815 - val_loss: 5134111.4479 - val_mae: 1914.0410\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1601381.8848 - mae: 917.3611 - val_loss: 1352777.1575 - val_mae: 952.5308\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1611605.7161 - mae: 926.4152 - val_loss: 1227717.7132 - val_mae: 672.4184\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1578597.7604 - mae: 914.5979 - val_loss: 1248709.2571 - val_mae: 674.3153\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1541920.4574 - mae: 892.0220 - val_loss: 2867333.6831 - val_mae: 1431.9783\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1610747.6039 - mae: 922.8382 - val_loss: 1275722.3504 - val_mae: 738.9528\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1557099.5480 - mae: 903.6985 - val_loss: 4063853.5101 - val_mae: 1704.5055\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1612413.0242 - mae: 920.4852 - val_loss: 1767693.3482 - val_mae: 892.2178\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1557081.5510 - mae: 899.5912 - val_loss: 1157234.2349 - val_mae: 833.3811\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1611288.9047 - mae: 923.7784 - val_loss: 1047255.1428 - val_mae: 775.1591\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1615529.7933 - mae: 917.8124 - val_loss: 1210340.3777 - val_mae: 889.3412\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1533410.2868 - mae: 895.3536 - val_loss: 1389203.1839 - val_mae: 983.1491\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1549496.3405 - mae: 899.0318 - val_loss: 1184983.8899 - val_mae: 671.6157\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1546846.7437 - mae: 896.9333 - val_loss: 1252897.3855 - val_mae: 682.2832\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1545268.5101 - mae: 903.8875 - val_loss: 2391208.4827 - val_mae: 1309.9216\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1531072.2555 - mae: 889.5113 - val_loss: 1000006.5003 - val_mae: 696.6897\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1540613.7475 - mae: 902.6946 - val_loss: 2198750.0774 - val_mae: 1033.0131\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1572573.1070 - mae: 905.2871 - val_loss: 1512727.1315 - val_mae: 753.1124\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1526135.3208 - mae: 888.3999 - val_loss: 1300418.5741 - val_mae: 709.1298\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1545273.5243 - mae: 899.7729 - val_loss: 2823611.7340 - val_mae: 1433.7687\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1554379.7840 - mae: 903.0335 - val_loss: 1169064.2815 - val_mae: 705.6010\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1557991.0108 - mae: 900.5303 - val_loss: 1072213.2031 - val_mae: 794.9980\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1506871.4446 - mae: 890.3366 - val_loss: 2099671.8997 - val_mae: 1007.8728\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1519435.7140 - mae: 895.7896 - val_loss: 1072865.0124 - val_mae: 658.1353\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1543385.8831 - mae: 897.6990 - val_loss: 1258523.7512 - val_mae: 697.5621\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1531570.1315 - mae: 886.1959 - val_loss: 1401391.2856 - val_mae: 989.6197\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1553719.6999 - mae: 905.2217 - val_loss: 2723776.6241 - val_mae: 1396.4174\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1544396.7904 - mae: 895.3082 - val_loss: 1181146.1849 - val_mae: 676.1887\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1518806.3809 - mae: 893.1187 - val_loss: 2272893.1102 - val_mae: 1266.0652\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1542929.2858 - mae: 890.2451 - val_loss: 1504519.3670 - val_mae: 762.5330\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1555015.9410 - mae: 897.7171 - val_loss: 1391181.8107 - val_mae: 718.9501\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1524002.5678 - mae: 893.9632 - val_loss: 3561783.8744 - val_mae: 1588.7844\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 1s 73us/step - loss: 1519314.6249 - mae: 886.8779 - val_loss: 1179666.5464 - val_mae: 682.7679\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1523802.3283 - mae: 890.0447 - val_loss: 1030964.9391 - val_mae: 686.3720\n",
      "Epoch 153/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 63us/step - loss: 1529542.0713 - mae: 892.2354 - val_loss: 1336275.2505 - val_mae: 931.2440\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1506654.3630 - mae: 885.5089 - val_loss: 1050504.7536 - val_mae: 791.3796\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1537450.6993 - mae: 895.3629 - val_loss: 1724887.5630 - val_mae: 1126.4716\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1504789.7668 - mae: 885.1793 - val_loss: 1677012.7520 - val_mae: 1073.0835\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1499727.7159 - mae: 885.4006 - val_loss: 1106883.2264 - val_mae: 806.1072\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1528766.3238 - mae: 889.2260 - val_loss: 1239761.6417 - val_mae: 677.3240\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1504985.2637 - mae: 886.7024 - val_loss: 1033702.1069 - val_mae: 751.6607\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1485625.5741 - mae: 875.8051 - val_loss: 1369946.1258 - val_mae: 707.3375\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 78us/step - loss: 5313854.2819 - mae: 1882.2393 - val_loss: 4474570.1922 - val_mae: 1704.9730\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 4513680.5070 - mae: 1708.4973 - val_loss: 4351899.2079 - val_mae: 1671.1271\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 4396166.0743 - mae: 1682.2660 - val_loss: 4206510.6263 - val_mae: 1642.3455\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 4173893.2034 - mae: 1629.7540 - val_loss: 4191302.6325 - val_mae: 1600.0184\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3990948.0815 - mae: 1583.0945 - val_loss: 3824502.2971 - val_mae: 1559.3923\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3837169.7716 - mae: 1557.1542 - val_loss: 3803649.7041 - val_mae: 1517.0997\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3723996.9104 - mae: 1532.9211 - val_loss: 3651493.6426 - val_mae: 1509.6200\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3667314.3453 - mae: 1530.4934 - val_loss: 3549675.7048 - val_mae: 1524.8088\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 3637981.2691 - mae: 1524.6030 - val_loss: 3519981.5170 - val_mae: 1530.6235\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 1s 74us/step - loss: 3579024.1745 - mae: 1513.3346 - val_loss: 3550280.4969 - val_mae: 1498.4834\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 3593962.3956 - mae: 1521.7291 - val_loss: 3517043.7739 - val_mae: 1497.0092\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 3526362.4571 - mae: 1497.5676 - val_loss: 3516231.2381 - val_mae: 1508.8778\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 3495420.9036 - mae: 1496.8806 - val_loss: 3460655.6630 - val_mae: 1509.5626\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 3499920.6950 - mae: 1501.8804 - val_loss: 3583953.0182 - val_mae: 1467.9922\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3471356.7966 - mae: 1493.8939 - val_loss: 3367213.9651 - val_mae: 1480.2307\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3398694.3291 - mae: 1481.0389 - val_loss: 3360095.4372 - val_mae: 1454.9379\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3359993.1595 - mae: 1467.7693 - val_loss: 3274653.5927 - val_mae: 1449.1846\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 3321375.5796 - mae: 1456.3412 - val_loss: 3202416.5138 - val_mae: 1468.9408\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 3180156.3214 - mae: 1419.1492 - val_loss: 3204879.4774 - val_mae: 1490.0197\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3102493.5630 - mae: 1396.8881 - val_loss: 2892446.5942 - val_mae: 1377.3223\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2964593.9923 - mae: 1358.5209 - val_loss: 2781548.5025 - val_mae: 1338.7493\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2795594.0828 - mae: 1313.1117 - val_loss: 2628030.7271 - val_mae: 1317.4598\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2662611.8214 - mae: 1263.1722 - val_loss: 2493095.2283 - val_mae: 1187.8710\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2585308.9448 - mae: 1239.8271 - val_loss: 2386351.9256 - val_mae: 1201.8324\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2538073.8165 - mae: 1228.7704 - val_loss: 2254719.4881 - val_mae: 1154.3297\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2484154.4265 - mae: 1208.1345 - val_loss: 2244130.9504 - val_mae: 1149.7728\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2450364.9625 - mae: 1199.6816 - val_loss: 2782978.3244 - val_mae: 1175.3544\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2438652.3870 - mae: 1195.2126 - val_loss: 2318156.9504 - val_mae: 1254.3573\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2391104.5882 - mae: 1181.5261 - val_loss: 2158963.0820 - val_mae: 1147.9803\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2368484.9526 - mae: 1174.3812 - val_loss: 2312988.2349 - val_mae: 1086.1722\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2344245.6595 - mae: 1169.5449 - val_loss: 2234851.8326 - val_mae: 1204.5391\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 1s 73us/step - loss: 2378552.7742 - mae: 1174.1644 - val_loss: 2087829.7594 - val_mae: 1093.9803\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2372131.8797 - mae: 1179.3645 - val_loss: 2216009.8904 - val_mae: 1135.2594\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2388389.2899 - mae: 1179.7617 - val_loss: 2481012.1573 - val_mae: 1147.9125\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2310434.6397 - mae: 1163.4094 - val_loss: 2173360.1404 - val_mae: 1219.6359\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 2265027.3900 - mae: 1147.3350 - val_loss: 2031602.4285 - val_mae: 1067.4921\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 2320966.6795 - mae: 1162.1127 - val_loss: 2567379.8998 - val_mae: 1124.4504\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2246882.8196 - mae: 1141.3990 - val_loss: 2408233.9011 - val_mae: 1294.5155\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 1s 95us/step - loss: 2233091.0014 - mae: 1146.0461 - val_loss: 2550874.3624 - val_mae: 1131.8135\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 1s 75us/step - loss: 2208210.4613 - mae: 1133.3925 - val_loss: 2128335.9548 - val_mae: 1203.2646\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2270410.8582 - mae: 1143.6879 - val_loss: 2368093.6687 - val_mae: 1275.6178\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 2233159.4255 - mae: 1140.1652 - val_loss: 1932638.6779 - val_mae: 1049.0065\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2178501.8026 - mae: 1126.3540 - val_loss: 2771059.4262 - val_mae: 1186.9645\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 2188879.8971 - mae: 1130.0750 - val_loss: 2066347.4193 - val_mae: 1016.0192\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2148912.3849 - mae: 1118.5150 - val_loss: 2590261.5656 - val_mae: 1362.5382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2154715.9343 - mae: 1114.1777 - val_loss: 2010876.4752 - val_mae: 1153.8260\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 2129078.8633 - mae: 1105.7804 - val_loss: 2564153.1894 - val_mae: 1353.3119\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2124649.3061 - mae: 1107.7618 - val_loss: 1819510.2899 - val_mae: 1083.5001\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2111877.2161 - mae: 1098.9498 - val_loss: 2440005.9758 - val_mae: 1100.6479\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 2108907.8084 - mae: 1097.0963 - val_loss: 2314923.0914 - val_mae: 1289.2631\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2054971.3076 - mae: 1083.8566 - val_loss: 2745976.7447 - val_mae: 1185.1495\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2039012.6143 - mae: 1075.4906 - val_loss: 1736659.5542 - val_mae: 933.3555\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2006198.5594 - mae: 1066.8788 - val_loss: 1881958.2750 - val_mae: 951.7313\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1962606.0986 - mae: 1056.1730 - val_loss: 1973845.5741 - val_mae: 960.4545\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1951914.1261 - mae: 1049.7969 - val_loss: 1577718.2985 - val_mae: 962.9459\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1924282.3968 - mae: 1040.5687 - val_loss: 3363138.1834 - val_mae: 1546.3458\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1935710.2590 - mae: 1040.9718 - val_loss: 1497429.0350 - val_mae: 926.9906\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1898435.6989 - mae: 1029.2083 - val_loss: 2114015.7032 - val_mae: 1211.4963\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1888302.7074 - mae: 1022.5504 - val_loss: 1589365.0358 - val_mae: 860.0974\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1870793.2791 - mae: 1017.1317 - val_loss: 1984202.0518 - val_mae: 957.1993\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1843239.4416 - mae: 1009.4374 - val_loss: 1474480.8459 - val_mae: 963.5825\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1798520.0208 - mae: 990.3472 - val_loss: 1518186.1597 - val_mae: 998.7111\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1744542.1540 - mae: 977.9433 - val_loss: 2028218.9086 - val_mae: 1195.1328\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1785593.5163 - mae: 986.5319 - val_loss: 2650686.6435 - val_mae: 1149.2603\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1750986.2633 - mae: 976.4092 - val_loss: 1643176.2604 - val_mae: 832.6465\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1750987.9679 - mae: 973.5881 - val_loss: 1313221.7467 - val_mae: 783.0612\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1752388.2196 - mae: 975.4965 - val_loss: 1362887.2817 - val_mae: 792.8195\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1697936.5951 - mae: 959.0349 - val_loss: 1405874.1686 - val_mae: 786.8174\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1717344.2127 - mae: 962.9670 - val_loss: 1518941.9124 - val_mae: 785.2068\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1713178.7476 - mae: 955.0585 - val_loss: 1670635.2398 - val_mae: 821.9811\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1722407.0187 - mae: 959.2319 - val_loss: 1591900.5733 - val_mae: 791.8590\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1731376.1470 - mae: 961.2031 - val_loss: 1329088.7521 - val_mae: 744.6395\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1732756.2698 - mae: 958.8627 - val_loss: 2090313.7352 - val_mae: 1216.1506\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1700429.8266 - mae: 955.9492 - val_loss: 1266236.8949 - val_mae: 834.9706\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1701207.8861 - mae: 946.4255 - val_loss: 1616019.8832 - val_mae: 819.0708\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1711307.1173 - mae: 960.4606 - val_loss: 1227348.0388 - val_mae: 838.0664\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1712633.8949 - mae: 953.0869 - val_loss: 1236748.6525 - val_mae: 842.2057\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1657843.2533 - mae: 940.6598 - val_loss: 1313092.3547 - val_mae: 798.1693\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1654847.6126 - mae: 936.4623 - val_loss: 1322527.2214 - val_mae: 741.9884\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1687660.2490 - mae: 947.8655 - val_loss: 2144531.9863 - val_mae: 1027.8236\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1673967.5303 - mae: 950.4648 - val_loss: 1177806.3123 - val_mae: 732.0743\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1665307.7734 - mae: 942.3480 - val_loss: 1509290.8024 - val_mae: 779.0593\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1650868.9239 - mae: 938.6100 - val_loss: 1464887.9454 - val_mae: 939.9013\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1659866.9019 - mae: 938.6540 - val_loss: 2121063.9351 - val_mae: 1011.8766\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1675423.1050 - mae: 944.0836 - val_loss: 1725223.8208 - val_mae: 856.3615\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1652669.0932 - mae: 944.9612 - val_loss: 1254376.1660 - val_mae: 700.2637\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1661099.7611 - mae: 937.7535 - val_loss: 1215971.5043 - val_mae: 699.6049\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1636884.4075 - mae: 938.7206 - val_loss: 2348689.8200 - val_mae: 1064.9583\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1653713.7799 - mae: 941.8024 - val_loss: 1800700.9342 - val_mae: 891.7297\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1659876.8549 - mae: 938.7235 - val_loss: 2415043.8854 - val_mae: 1101.7330\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1675113.9108 - mae: 942.8824 - val_loss: 1618860.0192 - val_mae: 818.7458\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1655185.4579 - mae: 940.0773 - val_loss: 1243858.5382 - val_mae: 822.4356\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1619935.8006 - mae: 929.3226 - val_loss: 2030820.5845 - val_mae: 1173.8094\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1621381.8448 - mae: 931.8824 - val_loss: 1486245.2682 - val_mae: 957.0463\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1669000.9279 - mae: 941.3712 - val_loss: 1930577.9084 - val_mae: 899.1760\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1644187.5417 - mae: 928.7859 - val_loss: 1524378.2155 - val_mae: 949.4742\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1653355.0745 - mae: 932.7832 - val_loss: 1159081.0451 - val_mae: 805.8426\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1629437.1078 - mae: 930.4072 - val_loss: 2150600.2222 - val_mae: 997.5337\n",
      "Epoch 99/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 55us/step - loss: 1641421.7456 - mae: 929.5067 - val_loss: 2118490.7428 - val_mae: 1215.6666\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1668832.4829 - mae: 939.7549 - val_loss: 1190234.4210 - val_mae: 850.3895\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1645469.8854 - mae: 929.4360 - val_loss: 1405712.6812 - val_mae: 747.9344\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1624963.0160 - mae: 929.6046 - val_loss: 1302939.5575 - val_mae: 702.0200\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1609237.1714 - mae: 921.9863 - val_loss: 1239691.6013 - val_mae: 696.2384\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1688336.6395 - mae: 944.0848 - val_loss: 1372296.1987 - val_mae: 720.0034\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1624525.7636 - mae: 930.7127 - val_loss: 2373108.0430 - val_mae: 1103.7706\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1602374.0183 - mae: 918.3255 - val_loss: 1174031.3719 - val_mae: 851.9144\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1605778.9442 - mae: 921.1535 - val_loss: 1458416.6930 - val_mae: 737.2730\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1623025.0133 - mae: 926.2130 - val_loss: 1264362.5467 - val_mae: 699.2221\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1596211.2895 - mae: 918.4489 - val_loss: 1344200.2115 - val_mae: 712.0672\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1623476.8180 - mae: 923.2961 - val_loss: 1184348.3745 - val_mae: 701.9104\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1606005.3101 - mae: 921.7018 - val_loss: 1926182.4932 - val_mae: 906.0160\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1633814.9579 - mae: 932.3959 - val_loss: 4217942.4328 - val_mae: 1754.9152\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1655539.8145 - mae: 930.5792 - val_loss: 2798969.9089 - val_mae: 1226.4344\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1593086.3177 - mae: 920.2696 - val_loss: 1297391.5126 - val_mae: 898.6374\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1603655.6847 - mae: 916.3452 - val_loss: 1124870.1707 - val_mae: 810.5751\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1618569.6595 - mae: 927.1772 - val_loss: 1862227.2613 - val_mae: 1134.5643\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1614380.4234 - mae: 929.9619 - val_loss: 1679125.7838 - val_mae: 842.1200\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1627886.4826 - mae: 921.7600 - val_loss: 1231984.0683 - val_mae: 890.7364\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1617992.2764 - mae: 928.8417 - val_loss: 1119018.5657 - val_mae: 780.7316\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1591608.8543 - mae: 914.7568 - val_loss: 1449045.8560 - val_mae: 957.3397\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1615886.0900 - mae: 922.1224 - val_loss: 1486544.3903 - val_mae: 749.0734\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1605955.1397 - mae: 916.3072 - val_loss: 1250972.6157 - val_mae: 683.3496\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1577736.7068 - mae: 911.1891 - val_loss: 3213719.4447 - val_mae: 1516.6893\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1621772.4624 - mae: 921.8637 - val_loss: 1382223.5725 - val_mae: 722.7993\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1589975.8847 - mae: 910.5737 - val_loss: 1323229.3299 - val_mae: 883.4299\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1583435.8052 - mae: 911.3300 - val_loss: 1925620.4061 - val_mae: 918.4444\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1621523.9098 - mae: 915.7308 - val_loss: 1179363.9943 - val_mae: 791.4293\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1587643.0413 - mae: 915.7492 - val_loss: 1914110.6760 - val_mae: 938.7645\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1606310.0684 - mae: 916.8196 - val_loss: 1531580.3423 - val_mae: 756.8695\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 52us/step - loss: 1590901.6339 - mae: 919.9609 - val_loss: 1110720.6971 - val_mae: 711.1420\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1589870.8400 - mae: 913.9860 - val_loss: 1068032.2402 - val_mae: 739.8499\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1595805.5255 - mae: 912.6165 - val_loss: 1326621.8899 - val_mae: 889.4795\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1610178.0319 - mae: 918.5423 - val_loss: 2712977.2283 - val_mae: 1398.3918\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1558264.2821 - mae: 908.6034 - val_loss: 1906388.1432 - val_mae: 1157.8856\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1578605.7702 - mae: 902.8300 - val_loss: 1344731.4359 - val_mae: 919.2878\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1580417.1326 - mae: 910.2961 - val_loss: 4056871.1646 - val_mae: 1724.4359\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1573542.6401 - mae: 906.9982 - val_loss: 1662573.1192 - val_mae: 824.1748\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1539955.8135 - mae: 893.2642 - val_loss: 2031473.6946 - val_mae: 966.4366\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1568893.6020 - mae: 912.5032 - val_loss: 1443883.0923 - val_mae: 733.2743\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1539488.5428 - mae: 901.4984 - val_loss: 1080841.4696 - val_mae: 747.8405\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1568409.8961 - mae: 910.1270 - val_loss: 1128734.5576 - val_mae: 676.2480\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1538461.4278 - mae: 901.0683 - val_loss: 1398439.4520 - val_mae: 750.5513\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1566688.9407 - mae: 898.8489 - val_loss: 1048711.2297 - val_mae: 698.5562\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 53us/step - loss: 1560349.5247 - mae: 904.9874 - val_loss: 1704856.6701 - val_mae: 1083.8085\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1580489.3755 - mae: 905.0416 - val_loss: 1760585.7123 - val_mae: 1109.1460\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1584828.8662 - mae: 913.3758 - val_loss: 1370912.5878 - val_mae: 958.7719\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 1555245.6756 - mae: 904.8210 - val_loss: 1070204.2172 - val_mae: 766.8648\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1585230.9105 - mae: 910.2103 - val_loss: 2155015.9092 - val_mae: 1234.4149\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 1s 74us/step - loss: 1572208.5764 - mae: 901.9642 - val_loss: 1895790.4428 - val_mae: 1162.1398\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1599720.6756 - mae: 912.4619 - val_loss: 1153384.8022 - val_mae: 753.7464\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1554690.9001 - mae: 899.4562 - val_loss: 1324563.4597 - val_mae: 709.0403\n",
      "Epoch 152/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 58us/step - loss: 1559657.9240 - mae: 903.4907 - val_loss: 1246443.7004 - val_mae: 858.5035\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1549825.2115 - mae: 900.8747 - val_loss: 1308173.4446 - val_mae: 707.7989\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1542069.4833 - mae: 894.7281 - val_loss: 1709561.9692 - val_mae: 861.2209\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1516295.8943 - mae: 893.5699 - val_loss: 1862742.8521 - val_mae: 1152.8853\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1505256.9428 - mae: 892.2986 - val_loss: 2193261.3153 - val_mae: 1260.9097\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1509789.1229 - mae: 889.6201 - val_loss: 1119466.9052 - val_mae: 706.0067\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 0s 54us/step - loss: 1566030.9774 - mae: 905.8563 - val_loss: 1380551.8969 - val_mae: 708.9808\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1530798.7569 - mae: 887.7009 - val_loss: 1527900.3693 - val_mae: 1006.3721\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1561204.3934 - mae: 895.6381 - val_loss: 1510163.0644 - val_mae: 752.2870\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 79us/step - loss: 5192454.2771 - mae: 1850.1592 - val_loss: 3778863.4372 - val_mae: 1555.3342\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 4594011.2046 - mae: 1728.5262 - val_loss: 3723037.8379 - val_mae: 1531.3849\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 4398390.5483 - mae: 1677.5314 - val_loss: 3642917.9604 - val_mae: 1500.1929\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 4252221.4321 - mae: 1642.6555 - val_loss: 3400484.9589 - val_mae: 1445.5735\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3962029.0658 - mae: 1582.0914 - val_loss: 3215416.2940 - val_mae: 1420.8452\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3821054.3501 - mae: 1552.7917 - val_loss: 3159525.5105 - val_mae: 1416.8997\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3742456.5982 - mae: 1543.5520 - val_loss: 3106910.0016 - val_mae: 1439.5243\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3691504.1220 - mae: 1537.6499 - val_loss: 3279058.9392 - val_mae: 1411.8087\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3707706.8910 - mae: 1543.0511 - val_loss: 3096538.8631 - val_mae: 1478.1945\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3648489.9947 - mae: 1530.4167 - val_loss: 3033747.5909 - val_mae: 1426.2593\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3620076.0731 - mae: 1524.9851 - val_loss: 3145562.5647 - val_mae: 1440.9945\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3606200.7542 - mae: 1524.3671 - val_loss: 3062247.1847 - val_mae: 1420.3921\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 3545535.1251 - mae: 1511.0081 - val_loss: 3016204.7753 - val_mae: 1408.6990\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3517904.1302 - mae: 1498.4705 - val_loss: 2969779.3576 - val_mae: 1389.5577\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3432255.4994 - mae: 1488.3788 - val_loss: 3050802.9001 - val_mae: 1460.4152\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 3420844.9496 - mae: 1484.7360 - val_loss: 2814732.4243 - val_mae: 1383.2997\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 3346624.9989 - mae: 1459.3490 - val_loss: 2776141.4750 - val_mae: 1339.4009\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3238318.4038 - mae: 1431.8153 - val_loss: 2771138.8466 - val_mae: 1333.3420\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 3132756.7041 - mae: 1404.8723 - val_loss: 2519150.4150 - val_mae: 1275.1890\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2955470.8495 - mae: 1358.6228 - val_loss: 2402417.7654 - val_mae: 1256.1053\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2854641.4343 - mae: 1324.7985 - val_loss: 2293385.6305 - val_mae: 1129.8940\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2669848.3887 - mae: 1263.4645 - val_loss: 2169729.2522 - val_mae: 1100.3010\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2639433.6619 - mae: 1253.7875 - val_loss: 2821735.5041 - val_mae: 1439.3439\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2578496.9692 - mae: 1237.4353 - val_loss: 2025860.2809 - val_mae: 1059.0985\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2521073.3925 - mae: 1226.2814 - val_loss: 2239985.1051 - val_mae: 1053.1125\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2483526.6392 - mae: 1218.4669 - val_loss: 1988201.2528 - val_mae: 1132.9879\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2503428.2513 - mae: 1217.4188 - val_loss: 2471513.1665 - val_mae: 1335.0544\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2417027.4377 - mae: 1198.3481 - val_loss: 1990777.9976 - val_mae: 1017.3002\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2406699.4698 - mae: 1195.2236 - val_loss: 2070377.3486 - val_mae: 1185.0829\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2410737.8299 - mae: 1192.5092 - val_loss: 2045933.9133 - val_mae: 1179.0371\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2407527.3633 - mae: 1184.3926 - val_loss: 1836320.5828 - val_mae: 1066.9727\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2373720.4516 - mae: 1183.8629 - val_loss: 2023851.1099 - val_mae: 1160.7649\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2312180.9141 - mae: 1168.0173 - val_loss: 1854482.7291 - val_mae: 1087.6071\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2328737.2446 - mae: 1166.9805 - val_loss: 1828659.5069 - val_mae: 1085.3827\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2327216.8356 - mae: 1160.5575 - val_loss: 1879984.6107 - val_mae: 1012.6544\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2332602.8288 - mae: 1169.2355 - val_loss: 1767147.8591 - val_mae: 1035.7864\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2278320.7918 - mae: 1156.2678 - val_loss: 1989051.1884 - val_mae: 979.6304\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2258749.6212 - mae: 1150.3832 - val_loss: 1806122.4340 - val_mae: 971.1560\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2220498.4650 - mae: 1136.6737 - val_loss: 1787531.8665 - val_mae: 1102.8108\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2216137.7136 - mae: 1137.7401 - val_loss: 2178327.0132 - val_mae: 1026.3165\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 1s 77us/step - loss: 2197195.4215 - mae: 1135.3351 - val_loss: 1679465.7528 - val_mae: 1020.5009\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2180661.8993 - mae: 1130.2443 - val_loss: 3074226.7054 - val_mae: 1512.2085\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2199976.7138 - mae: 1128.5529 - val_loss: 1616744.2242 - val_mae: 1001.9950\n",
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2177433.7888 - mae: 1128.0095 - val_loss: 1808323.2032 - val_mae: 930.0861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2112366.9814 - mae: 1107.3779 - val_loss: 1588966.3643 - val_mae: 999.6373\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 2106805.8714 - mae: 1098.4276 - val_loss: 2225014.7638 - val_mae: 1283.6813\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2107459.6611 - mae: 1104.2035 - val_loss: 1678204.1787 - val_mae: 1087.3615\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2088613.8209 - mae: 1084.3796 - val_loss: 1461173.1512 - val_mae: 929.3284\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2059450.0710 - mae: 1080.6121 - val_loss: 1643529.7432 - val_mae: 1065.1857\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2020467.8157 - mae: 1065.5090 - val_loss: 1596428.9073 - val_mae: 884.9809\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2018189.6615 - mae: 1076.8125 - val_loss: 2656718.8006 - val_mae: 1420.0751\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1991068.3531 - mae: 1063.6774 - val_loss: 5869617.3467 - val_mae: 2068.1497\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1983473.0281 - mae: 1058.0784 - val_loss: 1938518.2384 - val_mae: 960.3314\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1934496.2241 - mae: 1042.0732 - val_loss: 1627020.1336 - val_mae: 858.1852\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1890099.6786 - mae: 1030.9814 - val_loss: 1239214.4603 - val_mae: 868.9102\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1890935.6109 - mae: 1027.6721 - val_loss: 1327736.9954 - val_mae: 960.7949\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1820108.8414 - mae: 1005.7776 - val_loss: 1323081.0898 - val_mae: 778.0555\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1830778.8803 - mae: 1004.4962 - val_loss: 1252263.6611 - val_mae: 854.4531\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1809529.6736 - mae: 993.7059 - val_loss: 1135193.2454 - val_mae: 756.9838\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1805001.0365 - mae: 997.5295 - val_loss: 1080569.4161 - val_mae: 762.7932\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1789603.5136 - mae: 987.2010 - val_loss: 2395497.7811 - val_mae: 1356.2999\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1819002.5816 - mae: 993.3957 - val_loss: 1317565.8761 - val_mae: 974.1907\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1760671.8891 - mae: 981.3397 - val_loss: 973708.4272 - val_mae: 744.5135\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1773506.7464 - mae: 983.3478 - val_loss: 2112424.2282 - val_mae: 1039.0270\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1775014.0560 - mae: 973.7186 - val_loss: 1853797.1065 - val_mae: 978.4412\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1765624.6455 - mae: 976.6789 - val_loss: 3059505.4981 - val_mae: 1489.4869\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1745895.1125 - mae: 973.4099 - val_loss: 1064763.6228 - val_mae: 829.4239\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1763430.7596 - mae: 974.9467 - val_loss: 3419197.0867 - val_mae: 1567.6365\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1782285.7271 - mae: 978.7338 - val_loss: 1574860.8722 - val_mae: 830.4901\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1722386.2001 - mae: 963.6310 - val_loss: 1827173.1542 - val_mae: 1160.7074\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1769638.8387 - mae: 974.3837 - val_loss: 1030220.2459 - val_mae: 712.8418\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1753138.6049 - mae: 968.9423 - val_loss: 1172200.6427 - val_mae: 844.4964\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1742348.1873 - mae: 965.8373 - val_loss: 1263515.6424 - val_mae: 933.6085\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1710730.5853 - mae: 959.3104 - val_loss: 979344.8679 - val_mae: 711.4314\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1658700.1352 - mae: 945.4268 - val_loss: 1292911.8518 - val_mae: 974.0061\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1722850.9648 - mae: 958.6293 - val_loss: 1628558.7535 - val_mae: 1082.0428\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1740870.3766 - mae: 966.3044 - val_loss: 3090505.7180 - val_mae: 1399.2783\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1722031.0455 - mae: 959.2683 - val_loss: 950518.1690 - val_mae: 726.1242\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1754883.0827 - mae: 962.0357 - val_loss: 4049762.9786 - val_mae: 1723.0305\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1724689.7788 - mae: 958.0408 - val_loss: 1084291.1372 - val_mae: 716.2141\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1713013.7959 - mae: 954.9877 - val_loss: 2822417.8643 - val_mae: 1318.2621\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1678091.0477 - mae: 947.2426 - val_loss: 1108157.2005 - val_mae: 672.7261\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1663449.1386 - mae: 945.8854 - val_loss: 897671.5938 - val_mae: 674.3875\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1698625.7808 - mae: 946.0529 - val_loss: 4126554.5546 - val_mae: 1735.8800\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1661286.0339 - mae: 944.1387 - val_loss: 1647011.9381 - val_mae: 1121.2806\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1706330.4176 - mae: 952.5891 - val_loss: 3729260.6947 - val_mae: 1657.6638\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1688374.2387 - mae: 946.8649 - val_loss: 1270606.7209 - val_mae: 944.4757\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1688169.8819 - mae: 945.2509 - val_loss: 1227259.8188 - val_mae: 682.9294\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 1s 75us/step - loss: 1684418.4950 - mae: 946.3610 - val_loss: 924465.4638 - val_mae: 727.4291\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 70us/step - loss: 1695215.5453 - mae: 943.1045 - val_loss: 955640.8996 - val_mae: 772.2308\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1700925.4526 - mae: 951.0981 - val_loss: 1625016.8348 - val_mae: 889.6418\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1681433.1072 - mae: 943.6899 - val_loss: 928625.5779 - val_mae: 756.1237\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1660603.4798 - mae: 937.9197 - val_loss: 1196432.5976 - val_mae: 668.8303\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1693108.8565 - mae: 948.2774 - val_loss: 888178.5484 - val_mae: 684.7070\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1696720.4168 - mae: 949.5428 - val_loss: 1267095.0079 - val_mae: 717.1924\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1698704.8589 - mae: 947.0568 - val_loss: 3640505.0967 - val_mae: 1655.6931\n",
      "Epoch 97/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1706883.2428 - mae: 951.5526 - val_loss: 1947841.7475 - val_mae: 988.4352\n",
      "Epoch 98/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 63us/step - loss: 1645430.6085 - mae: 936.1391 - val_loss: 965868.6748 - val_mae: 772.7756\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1664983.6920 - mae: 942.6437 - val_loss: 898381.0800 - val_mae: 730.8427\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1642640.4422 - mae: 937.2353 - val_loss: 904539.2591 - val_mae: 691.3408\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1677513.3505 - mae: 946.7603 - val_loss: 889657.6091 - val_mae: 616.0330\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1674100.3361 - mae: 943.5021 - val_loss: 2252051.7252 - val_mae: 1301.5853\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1671843.3050 - mae: 932.6219 - val_loss: 1105242.6460 - val_mae: 871.1636\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1682663.1666 - mae: 949.0599 - val_loss: 920380.0201 - val_mae: 618.7808\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1635554.3133 - mae: 931.5045 - val_loss: 3122248.9447 - val_mae: 1530.8290\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1671276.5464 - mae: 935.0758 - val_loss: 2002530.0408 - val_mae: 1241.7369\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1632361.6507 - mae: 930.4514 - val_loss: 1203280.0243 - val_mae: 687.0132\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1690799.3200 - mae: 942.3964 - val_loss: 1009352.3568 - val_mae: 824.0057\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1618517.4574 - mae: 923.6396 - val_loss: 905301.9133 - val_mae: 738.2872\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1610371.1489 - mae: 927.4427 - val_loss: 867423.2299 - val_mae: 663.0250\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1614523.9832 - mae: 929.5559 - val_loss: 1317808.5254 - val_mae: 751.9118\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1627079.0053 - mae: 930.7824 - val_loss: 1451890.3161 - val_mae: 807.1547\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1669749.1374 - mae: 937.0298 - val_loss: 996455.0463 - val_mae: 802.8613\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1619955.5363 - mae: 922.4786 - val_loss: 899683.0817 - val_mae: 756.0576\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1624640.2859 - mae: 923.8651 - val_loss: 1243725.4667 - val_mae: 687.1450\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1635657.0604 - mae: 932.8116 - val_loss: 1267528.4296 - val_mae: 941.2680\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1615384.9796 - mae: 926.6318 - val_loss: 923181.7811 - val_mae: 607.1100\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1623288.2406 - mae: 928.4731 - val_loss: 1089345.4323 - val_mae: 660.0074\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1613630.2360 - mae: 924.8284 - val_loss: 1365215.1347 - val_mae: 988.8356\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1611850.1801 - mae: 922.6401 - val_loss: 1936671.1665 - val_mae: 1202.8279\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1621515.5164 - mae: 920.6058 - val_loss: 847453.7397 - val_mae: 647.9969\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1666861.5826 - mae: 934.7665 - val_loss: 2524934.6944 - val_mae: 1376.3678\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1648385.2534 - mae: 930.5064 - val_loss: 1317758.3205 - val_mae: 742.1504\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1599532.7618 - mae: 921.5959 - val_loss: 901993.5496 - val_mae: 661.9699\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1638493.9474 - mae: 934.4873 - val_loss: 860822.4781 - val_mae: 620.5460\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1565490.9342 - mae: 906.9877 - val_loss: 806128.8596 - val_mae: 645.0203\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1612349.7375 - mae: 923.0499 - val_loss: 894772.3888 - val_mae: 585.8117\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1631772.2737 - mae: 930.0004 - val_loss: 1801940.7425 - val_mae: 956.0284\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1615432.4848 - mae: 915.8928 - val_loss: 1091424.7019 - val_mae: 642.9343\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1594294.0090 - mae: 918.3987 - val_loss: 1115506.9614 - val_mae: 900.5084\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1605907.9860 - mae: 923.3217 - val_loss: 1809081.2410 - val_mae: 944.0507\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1579619.4308 - mae: 906.2746 - val_loss: 1441023.4383 - val_mae: 811.4462\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1606721.6048 - mae: 921.9526 - val_loss: 1537021.1622 - val_mae: 829.1179\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1619921.4740 - mae: 924.9930 - val_loss: 2112880.7830 - val_mae: 1273.7230\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1583220.9525 - mae: 916.6702 - val_loss: 1340550.9498 - val_mae: 744.7199\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1616071.2162 - mae: 917.9910 - val_loss: 985094.1768 - val_mae: 820.3054\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1612564.9948 - mae: 916.9062 - val_loss: 1066532.0928 - val_mae: 635.3199\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1622277.6129 - mae: 923.6345 - val_loss: 1296111.4801 - val_mae: 970.2571\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1561355.6369 - mae: 911.1155 - val_loss: 1703277.5242 - val_mae: 1140.7239\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1581243.4881 - mae: 914.3250 - val_loss: 4156796.1338 - val_mae: 1775.5768\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1618757.3283 - mae: 919.6881 - val_loss: 862299.8949 - val_mae: 639.9204\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1592238.9834 - mae: 916.5775 - val_loss: 813757.3960 - val_mae: 651.9604\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1571762.2736 - mae: 912.9309 - val_loss: 3775085.0848 - val_mae: 1669.9548\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1623568.9046 - mae: 917.3663 - val_loss: 1708038.1969 - val_mae: 1133.5322\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1574270.1546 - mae: 908.0771 - val_loss: 839158.1595 - val_mae: 692.8915\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1585865.2310 - mae: 917.3220 - val_loss: 1544682.7217 - val_mae: 1072.9017\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1581126.2685 - mae: 907.8301 - val_loss: 1516340.8998 - val_mae: 824.3355\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1586306.0919 - mae: 909.4898 - val_loss: 934969.3574 - val_mae: 677.1759\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1576116.1052 - mae: 911.3986 - val_loss: 1434698.8659 - val_mae: 1031.1029\n",
      "Epoch 150/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1571060.3876 - mae: 910.6868 - val_loss: 1830753.4249 - val_mae: 1161.5254\n",
      "Epoch 151/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 58us/step - loss: 1599214.7587 - mae: 916.9855 - val_loss: 1760258.3640 - val_mae: 949.9617\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1577605.1895 - mae: 908.0160 - val_loss: 1392050.5060 - val_mae: 760.8983\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1561229.0409 - mae: 904.7252 - val_loss: 1135598.2034 - val_mae: 647.1059\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1617262.2577 - mae: 922.6370 - val_loss: 823694.6247 - val_mae: 642.1165\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1555730.1364 - mae: 900.7844 - val_loss: 1145782.9056 - val_mae: 641.7034\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1581807.6346 - mae: 907.1119 - val_loss: 942098.2937 - val_mae: 665.3937\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1532420.5767 - mae: 894.1941 - val_loss: 896218.8327 - val_mae: 602.2569\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1599108.9571 - mae: 917.0293 - val_loss: 1117471.6557 - val_mae: 651.5889\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1575867.9182 - mae: 904.3452 - val_loss: 858599.0538 - val_mae: 722.3930\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1561749.3678 - mae: 900.0715 - val_loss: 1111569.7915 - val_mae: 880.6360\n",
      "Train on 7172 samples, validate on 796 samples\n",
      "Epoch 1/160\n",
      "7172/7172 [==============================] - 1s 78us/step - loss: 5097850.9014 - mae: 1833.7860 - val_loss: 3882672.6143 - val_mae: 1612.8087\n",
      "Epoch 2/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 4601211.0004 - mae: 1732.0167 - val_loss: 3817852.6834 - val_mae: 1594.0959\n",
      "Epoch 3/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 4459513.9683 - mae: 1691.5914 - val_loss: 3949484.8417 - val_mae: 1605.7472\n",
      "Epoch 4/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 4296372.5545 - mae: 1652.0754 - val_loss: 3519218.7236 - val_mae: 1520.2976\n",
      "Epoch 5/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 4058519.7117 - mae: 1595.6478 - val_loss: 3323601.5798 - val_mae: 1466.8734\n",
      "Epoch 6/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 3880395.6394 - mae: 1557.0217 - val_loss: 3165228.5402 - val_mae: 1437.0389\n",
      "Epoch 7/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3774662.5825 - mae: 1543.2490 - val_loss: 3070733.3254 - val_mae: 1426.0133\n",
      "Epoch 8/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 3765307.1367 - mae: 1546.7521 - val_loss: 2979616.3197 - val_mae: 1440.7178\n",
      "Epoch 9/160\n",
      "7172/7172 [==============================] - 0s 55us/step - loss: 3655786.9804 - mae: 1529.5264 - val_loss: 3035538.9969 - val_mae: 1421.1677\n",
      "Epoch 10/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 3654679.1979 - mae: 1532.3906 - val_loss: 2930836.3907 - val_mae: 1421.6555\n",
      "Epoch 11/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 3639460.7862 - mae: 1524.7325 - val_loss: 2941579.4070 - val_mae: 1411.4719\n",
      "Epoch 12/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 3659968.0095 - mae: 1532.9406 - val_loss: 2881105.5352 - val_mae: 1403.6986\n",
      "Epoch 13/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 3623525.3067 - mae: 1528.9059 - val_loss: 3092891.1241 - val_mae: 1403.9722\n",
      "Epoch 14/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 3540997.4755 - mae: 1512.0021 - val_loss: 2841952.5521 - val_mae: 1389.6710\n",
      "Epoch 15/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 3520161.7209 - mae: 1504.8610 - val_loss: 2792859.9168 - val_mae: 1373.6528\n",
      "Epoch 16/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3470150.1138 - mae: 1493.2020 - val_loss: 2734988.1058 - val_mae: 1370.9930\n",
      "Epoch 17/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 3394760.2733 - mae: 1474.1827 - val_loss: 2681611.7544 - val_mae: 1348.2147\n",
      "Epoch 18/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 3356271.5760 - mae: 1466.4805 - val_loss: 2629451.1884 - val_mae: 1322.2650\n",
      "Epoch 19/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 3231033.4666 - mae: 1427.8235 - val_loss: 2509821.6897 - val_mae: 1299.2168\n",
      "Epoch 20/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 3119272.6120 - mae: 1404.4335 - val_loss: 2414565.5298 - val_mae: 1262.8281\n",
      "Epoch 21/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 3029508.1474 - mae: 1377.2565 - val_loss: 2299734.8894 - val_mae: 1238.6189\n",
      "Epoch 22/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2859544.7799 - mae: 1323.8503 - val_loss: 2117795.3342 - val_mae: 1162.2650\n",
      "Epoch 23/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2689044.2668 - mae: 1264.9601 - val_loss: 2027310.4220 - val_mae: 1109.1531\n",
      "Epoch 24/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 2619362.5900 - mae: 1246.0992 - val_loss: 2116937.7484 - val_mae: 1064.1251\n",
      "Epoch 25/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2548831.0527 - mae: 1222.3262 - val_loss: 2025979.5606 - val_mae: 1170.3236\n",
      "Epoch 26/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 2483923.4906 - mae: 1205.9048 - val_loss: 1850429.5612 - val_mae: 1051.9211\n",
      "Epoch 27/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2496409.5188 - mae: 1209.8859 - val_loss: 1873689.1071 - val_mae: 1104.1857\n",
      "Epoch 28/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2428726.1611 - mae: 1190.4634 - val_loss: 1811022.2508 - val_mae: 1075.0789\n",
      "Epoch 29/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2409490.1392 - mae: 1184.6624 - val_loss: 1872469.3312 - val_mae: 1112.1682\n",
      "Epoch 30/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2406961.5671 - mae: 1188.0038 - val_loss: 1831725.7255 - val_mae: 1098.6952\n",
      "Epoch 31/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2373299.0663 - mae: 1178.1636 - val_loss: 1742492.7742 - val_mae: 1042.9941\n",
      "Epoch 32/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2367519.1727 - mae: 1174.0959 - val_loss: 2375804.4273 - val_mae: 1113.4042\n",
      "Epoch 33/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2340758.3094 - mae: 1170.9183 - val_loss: 2330265.7817 - val_mae: 1287.7780\n",
      "Epoch 34/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2338265.6532 - mae: 1160.3910 - val_loss: 1998322.9906 - val_mae: 1027.2455\n",
      "Epoch 35/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2321298.2150 - mae: 1164.2290 - val_loss: 1785564.9421 - val_mae: 1095.5391\n",
      "Epoch 36/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2286209.5185 - mae: 1146.4434 - val_loss: 1690985.0251 - val_mae: 1032.3392\n",
      "Epoch 37/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 2306939.9873 - mae: 1151.6307 - val_loss: 1697042.0347 - val_mae: 1012.6426\n",
      "Epoch 38/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 2272237.8139 - mae: 1151.8188 - val_loss: 1767108.7968 - val_mae: 1102.3707\n",
      "Epoch 39/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2307441.3088 - mae: 1156.5786 - val_loss: 1758667.9202 - val_mae: 1094.0399\n",
      "Epoch 40/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 2208276.7105 - mae: 1131.1675 - val_loss: 1661071.6156 - val_mae: 1030.3240\n",
      "Epoch 41/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2223043.8118 - mae: 1134.7576 - val_loss: 1591473.5113 - val_mae: 952.2037\n",
      "Epoch 42/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2207337.7914 - mae: 1129.3733 - val_loss: 2172076.7260 - val_mae: 1067.0055\n",
      "Epoch 43/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2204659.1464 - mae: 1125.2644 - val_loss: 2054211.6999 - val_mae: 1025.5126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 2186184.2356 - mae: 1117.2201 - val_loss: 1587165.6438 - val_mae: 971.1362\n",
      "Epoch 45/160\n",
      "7172/7172 [==============================] - 1s 71us/step - loss: 2112034.6580 - mae: 1104.6118 - val_loss: 1584974.1534 - val_mae: 1022.7245\n",
      "Epoch 46/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2132203.1363 - mae: 1104.5370 - val_loss: 4510651.8505 - val_mae: 1782.3510\n",
      "Epoch 47/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2127391.9989 - mae: 1098.8385 - val_loss: 1971053.5091 - val_mae: 1004.8226\n",
      "Epoch 48/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 2044127.6142 - mae: 1078.3258 - val_loss: 1687557.0367 - val_mae: 935.0211\n",
      "Epoch 49/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 2076140.0736 - mae: 1084.8943 - val_loss: 1485162.8321 - val_mae: 927.5134\n",
      "Epoch 50/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2030029.2644 - mae: 1069.9958 - val_loss: 2236770.9625 - val_mae: 1081.6467\n",
      "Epoch 51/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 2020573.3081 - mae: 1064.6527 - val_loss: 1457406.2166 - val_mae: 867.0957\n",
      "Epoch 52/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 2029433.2867 - mae: 1063.0527 - val_loss: 1810393.9436 - val_mae: 948.8423\n",
      "Epoch 53/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1951499.1815 - mae: 1043.4423 - val_loss: 1295931.5765 - val_mae: 902.9164\n",
      "Epoch 54/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1943459.2852 - mae: 1041.8208 - val_loss: 1278138.2853 - val_mae: 844.6871\n",
      "Epoch 55/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1953271.9841 - mae: 1044.3843 - val_loss: 2034930.2966 - val_mae: 1018.2820\n",
      "Epoch 56/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1899973.6687 - mae: 1025.3120 - val_loss: 1479011.0454 - val_mae: 836.9619\n",
      "Epoch 57/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1853970.5235 - mae: 1012.8094 - val_loss: 1787339.2557 - val_mae: 942.9364\n",
      "Epoch 58/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1834030.7578 - mae: 1004.8422 - val_loss: 1246974.6944 - val_mae: 908.0685\n",
      "Epoch 59/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1825816.2415 - mae: 1000.7306 - val_loss: 1251636.4891 - val_mae: 800.8615\n",
      "Epoch 60/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1808146.4193 - mae: 991.0274 - val_loss: 1462726.1140 - val_mae: 1024.3472\n",
      "Epoch 61/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1798876.7564 - mae: 993.1991 - val_loss: 1106254.7556 - val_mae: 765.0702\n",
      "Epoch 62/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1743421.7111 - mae: 965.7349 - val_loss: 1510291.3219 - val_mae: 1035.4286\n",
      "Epoch 63/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1773694.9308 - mae: 973.7933 - val_loss: 1098584.1457 - val_mae: 759.7097\n",
      "Epoch 64/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1747648.0387 - mae: 969.8227 - val_loss: 1002636.1764 - val_mae: 754.6937\n",
      "Epoch 65/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1759707.6293 - mae: 972.2515 - val_loss: 1475770.6299 - val_mae: 1019.9074\n",
      "Epoch 66/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1713545.2001 - mae: 958.7965 - val_loss: 1045506.2250 - val_mae: 718.8156\n",
      "Epoch 67/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1779147.6966 - mae: 974.0652 - val_loss: 1252463.6079 - val_mae: 920.1551\n",
      "Epoch 68/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1718891.0769 - mae: 952.8562 - val_loss: 1009365.5293 - val_mae: 699.4353\n",
      "Epoch 69/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1733682.5219 - mae: 959.7104 - val_loss: 1026792.4111 - val_mae: 778.7069\n",
      "Epoch 70/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1720565.2637 - mae: 959.6516 - val_loss: 1595973.8080 - val_mae: 872.5208\n",
      "Epoch 71/160\n",
      "7172/7172 [==============================] - 0s 56us/step - loss: 1699061.1073 - mae: 945.9702 - val_loss: 1145570.4662 - val_mae: 688.5378\n",
      "Epoch 72/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1729517.0963 - mae: 964.3855 - val_loss: 1520945.6595 - val_mae: 1044.7084\n",
      "Epoch 73/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1705482.2842 - mae: 951.1147 - val_loss: 1124774.6028 - val_mae: 720.5689\n",
      "Epoch 74/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1723594.0371 - mae: 955.5866 - val_loss: 1351072.4311 - val_mae: 774.8923\n",
      "Epoch 75/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1693317.8546 - mae: 950.7834 - val_loss: 4008690.7676 - val_mae: 1683.0372\n",
      "Epoch 76/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1658978.1168 - mae: 940.5117 - val_loss: 1566216.7785 - val_mae: 870.4670\n",
      "Epoch 77/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1702876.9791 - mae: 947.9195 - val_loss: 1565471.7508 - val_mae: 855.2365\n",
      "Epoch 78/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1663276.5220 - mae: 930.8590 - val_loss: 1917408.9840 - val_mae: 1165.5620\n",
      "Epoch 79/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1640536.8823 - mae: 936.9706 - val_loss: 1232403.7940 - val_mae: 927.3333\n",
      "Epoch 80/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1711989.7913 - mae: 949.5253 - val_loss: 1008092.3318 - val_mae: 683.2798\n",
      "Epoch 81/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1715287.7723 - mae: 951.0250 - val_loss: 1087360.4795 - val_mae: 685.5960\n",
      "Epoch 82/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1723617.2962 - mae: 951.4824 - val_loss: 1024907.0287 - val_mae: 769.9251\n",
      "Epoch 83/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1717634.7489 - mae: 953.3185 - val_loss: 957858.7476 - val_mae: 774.4384\n",
      "Epoch 84/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1679580.2639 - mae: 942.9127 - val_loss: 1070929.1462 - val_mae: 727.2526\n",
      "Epoch 85/160\n",
      "7172/7172 [==============================] - 0s 69us/step - loss: 1677763.9809 - mae: 945.9500 - val_loss: 1427174.5598 - val_mae: 797.0486\n",
      "Epoch 86/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1733390.6380 - mae: 949.7317 - val_loss: 1012137.2875 - val_mae: 798.8193\n",
      "Epoch 87/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1680458.5089 - mae: 941.3337 - val_loss: 1763984.7633 - val_mae: 959.3475\n",
      "Epoch 88/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1724208.7466 - mae: 947.5969 - val_loss: 985524.0367 - val_mae: 731.7784\n",
      "Epoch 89/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1668924.4570 - mae: 940.6553 - val_loss: 954451.1844 - val_mae: 755.0519\n",
      "Epoch 90/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1687022.3326 - mae: 939.6949 - val_loss: 921559.9165 - val_mae: 694.8120\n",
      "Epoch 91/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1663678.5073 - mae: 931.6722 - val_loss: 1084172.3062 - val_mae: 674.2077\n",
      "Epoch 92/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1673882.6724 - mae: 942.0204 - val_loss: 980312.1864 - val_mae: 674.3283\n",
      "Epoch 93/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1647402.1189 - mae: 938.1173 - val_loss: 3701825.6665 - val_mae: 1631.4757\n",
      "Epoch 94/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1675283.0212 - mae: 937.5272 - val_loss: 1483074.8598 - val_mae: 1014.1236\n",
      "Epoch 95/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1674928.5660 - mae: 938.7165 - val_loss: 1009261.9400 - val_mae: 662.7028\n",
      "Epoch 96/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1653661.8226 - mae: 936.8587 - val_loss: 1388304.1350 - val_mae: 804.4069\n",
      "Epoch 97/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 56us/step - loss: 1653782.2752 - mae: 933.6172 - val_loss: 1039864.9931 - val_mae: 791.7737\n",
      "Epoch 98/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1605085.6502 - mae: 920.3126 - val_loss: 1480063.1785 - val_mae: 829.7980\n",
      "Epoch 99/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1643171.4496 - mae: 933.7534 - val_loss: 1649301.6336 - val_mae: 889.1500\n",
      "Epoch 100/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1622834.7015 - mae: 921.6704 - val_loss: 2246058.5540 - val_mae: 1144.8075\n",
      "Epoch 101/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1687837.9196 - mae: 937.6063 - val_loss: 964509.6698 - val_mae: 790.7258\n",
      "Epoch 102/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1611671.3330 - mae: 926.4024 - val_loss: 1188070.3232 - val_mae: 725.8054\n",
      "Epoch 103/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1679501.0978 - mae: 939.4106 - val_loss: 1121663.9680 - val_mae: 838.5989\n",
      "Epoch 104/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1631692.1545 - mae: 921.6501 - val_loss: 1006767.6657 - val_mae: 664.6019\n",
      "Epoch 105/160\n",
      "7172/7172 [==============================] - 0s 68us/step - loss: 1623830.1919 - mae: 926.5835 - val_loss: 984400.6101 - val_mae: 801.5741\n",
      "Epoch 106/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1582689.7671 - mae: 915.4321 - val_loss: 1010365.7765 - val_mae: 803.1905\n",
      "Epoch 107/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1618388.8860 - mae: 920.8411 - val_loss: 892293.9954 - val_mae: 702.1552\n",
      "Epoch 108/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1647995.1906 - mae: 932.3670 - val_loss: 902690.7943 - val_mae: 749.3246\n",
      "Epoch 109/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1629182.5834 - mae: 925.5649 - val_loss: 1860510.2908 - val_mae: 1162.6431\n",
      "Epoch 110/160\n",
      "7172/7172 [==============================] - 0s 63us/step - loss: 1626791.3781 - mae: 922.1429 - val_loss: 1084763.2129 - val_mae: 708.8180\n",
      "Epoch 111/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1622182.1158 - mae: 917.3698 - val_loss: 1508365.8714 - val_mae: 1052.6108\n",
      "Epoch 112/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1667953.9775 - mae: 935.4387 - val_loss: 962313.4141 - val_mae: 687.3953\n",
      "Epoch 113/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1594689.4415 - mae: 920.8100 - val_loss: 1318466.7894 - val_mae: 958.3031\n",
      "Epoch 114/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1607912.4453 - mae: 915.9476 - val_loss: 1040059.9667 - val_mae: 848.1842\n",
      "Epoch 115/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1601393.6775 - mae: 918.9014 - val_loss: 1193835.8885 - val_mae: 924.8279\n",
      "Epoch 116/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1595523.3169 - mae: 914.3786 - val_loss: 1051248.4837 - val_mae: 668.4842\n",
      "Epoch 117/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1606684.8195 - mae: 920.3359 - val_loss: 908008.3890 - val_mae: 701.0975\n",
      "Epoch 118/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1606980.1655 - mae: 918.4498 - val_loss: 1706293.2699 - val_mae: 922.9922\n",
      "Epoch 119/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1612915.5548 - mae: 913.4623 - val_loss: 1644126.9783 - val_mae: 1093.2515\n",
      "Epoch 120/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1579655.6752 - mae: 909.0629 - val_loss: 1410085.2856 - val_mae: 1019.4971\n",
      "Epoch 121/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1630505.9622 - mae: 928.4528 - val_loss: 875880.6694 - val_mae: 657.6747\n",
      "Epoch 122/160\n",
      "7172/7172 [==============================] - 0s 66us/step - loss: 1604369.8632 - mae: 911.1489 - val_loss: 1632667.6413 - val_mae: 903.5520\n",
      "Epoch 123/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1590578.5158 - mae: 913.1994 - val_loss: 3118155.9472 - val_mae: 1493.6912\n",
      "Epoch 124/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1620706.8628 - mae: 924.4293 - val_loss: 1269883.9593 - val_mae: 941.9206\n",
      "Epoch 125/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1594479.3446 - mae: 912.0338 - val_loss: 1065029.6471 - val_mae: 664.7766\n",
      "Epoch 126/160\n",
      "7172/7172 [==============================] - 0s 67us/step - loss: 1629794.2935 - mae: 925.8681 - val_loss: 1072687.7262 - val_mae: 667.0145\n",
      "Epoch 127/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1595609.1721 - mae: 916.5807 - val_loss: 1247340.0226 - val_mae: 725.4929\n",
      "Epoch 128/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1603430.3254 - mae: 916.7648 - val_loss: 1401654.9771 - val_mae: 998.3491\n",
      "Epoch 129/160\n",
      "7172/7172 [==============================] - 0s 64us/step - loss: 1620057.7524 - mae: 918.6406 - val_loss: 1454620.5747 - val_mae: 1015.5718\n",
      "Epoch 130/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1580049.7086 - mae: 906.4842 - val_loss: 1528648.0030 - val_mae: 856.1771\n",
      "Epoch 131/160\n",
      "7172/7172 [==============================] - 0s 65us/step - loss: 1590661.0367 - mae: 913.2333 - val_loss: 836106.5005 - val_mae: 678.6363\n",
      "Epoch 132/160\n",
      "7172/7172 [==============================] - 0s 60us/step - loss: 1589286.4620 - mae: 911.2569 - val_loss: 1201718.4103 - val_mae: 686.9916\n",
      "Epoch 133/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1568728.5962 - mae: 909.7293 - val_loss: 931813.3496 - val_mae: 671.8787\n",
      "Epoch 134/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1642268.3076 - mae: 923.1641 - val_loss: 1646371.1256 - val_mae: 1101.0903\n",
      "Epoch 135/160\n",
      "7172/7172 [==============================] - 0s 62us/step - loss: 1592202.6315 - mae: 914.4543 - val_loss: 957270.5461 - val_mae: 637.7301\n",
      "Epoch 136/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1591101.5630 - mae: 911.5906 - val_loss: 885777.4344 - val_mae: 657.7072\n",
      "Epoch 137/160\n",
      "7172/7172 [==============================] - 0s 61us/step - loss: 1586253.0214 - mae: 908.0361 - val_loss: 900893.1556 - val_mae: 634.0977\n",
      "Epoch 138/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1532603.4834 - mae: 896.0911 - val_loss: 1129483.5826 - val_mae: 877.2404\n",
      "Epoch 139/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1551435.7453 - mae: 899.5441 - val_loss: 1265449.7340 - val_mae: 953.3741\n",
      "Epoch 140/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1570946.4499 - mae: 907.9221 - val_loss: 1616011.5155 - val_mae: 879.3125\n",
      "Epoch 141/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1530303.3654 - mae: 890.0342 - val_loss: 924617.2919 - val_mae: 659.0230\n",
      "Epoch 142/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1586014.3542 - mae: 906.5199 - val_loss: 1186591.0367 - val_mae: 711.7114\n",
      "Epoch 143/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1572472.4836 - mae: 906.7441 - val_loss: 1081029.1591 - val_mae: 879.3004\n",
      "Epoch 144/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1552736.4980 - mae: 898.8046 - val_loss: 3249959.0977 - val_mae: 1531.1438\n",
      "Epoch 145/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1556049.8811 - mae: 897.3651 - val_loss: 935172.2225 - val_mae: 790.8049\n",
      "Epoch 146/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1547432.2658 - mae: 896.7110 - val_loss: 882172.2765 - val_mae: 618.7927\n",
      "Epoch 147/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1600186.7496 - mae: 917.7499 - val_loss: 1286109.4334 - val_mae: 971.6864\n",
      "Epoch 148/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1573626.9512 - mae: 904.9172 - val_loss: 920035.2839 - val_mae: 758.4367\n",
      "Epoch 149/160\n",
      "7172/7172 [==============================] - 0s 59us/step - loss: 1546046.7165 - mae: 896.2876 - val_loss: 889353.0188 - val_mae: 690.2941\n",
      "Epoch 150/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 57us/step - loss: 1556017.8331 - mae: 903.4641 - val_loss: 1114686.9538 - val_mae: 658.7584\n",
      "Epoch 151/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1511114.1466 - mae: 883.9842 - val_loss: 1347818.2462 - val_mae: 757.8256\n",
      "Epoch 152/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1526046.7100 - mae: 891.5568 - val_loss: 865966.8536 - val_mae: 667.1305\n",
      "Epoch 153/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1563631.2698 - mae: 900.1616 - val_loss: 952686.4810 - val_mae: 717.4216\n",
      "Epoch 154/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1563657.9208 - mae: 902.8208 - val_loss: 1061755.0726 - val_mae: 843.2961\n",
      "Epoch 155/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1564633.7660 - mae: 897.1558 - val_loss: 1344808.2966 - val_mae: 748.6129\n",
      "Epoch 156/160\n",
      "7172/7172 [==============================] - 0s 58us/step - loss: 1559553.0112 - mae: 893.8680 - val_loss: 2119543.7758 - val_mae: 1249.1648\n",
      "Epoch 157/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1558951.0336 - mae: 895.2545 - val_loss: 2427913.7503 - val_mae: 1321.0159\n",
      "Epoch 158/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1543158.4586 - mae: 895.1991 - val_loss: 1011792.2599 - val_mae: 646.6478\n",
      "Epoch 159/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1563623.1277 - mae: 899.8010 - val_loss: 1605858.5697 - val_mae: 888.7762\n",
      "Epoch 160/160\n",
      "7172/7172 [==============================] - 0s 57us/step - loss: 1540694.7162 - mae: 894.8087 - val_loss: 881843.1085 - val_mae: 694.7703\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(54,activation='relu',input_shape=(train_data.shape[1],),kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64,activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n",
    "    return model\n",
    "#下面k=10,num_epochs = 80意味着10个验证集，每个验证80次\n",
    "k = 10\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 160\n",
    "all_scores = []\n",
    "all_mae_history = []\n",
    "#交叉验证\n",
    "for i in range(k):\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_label = train_label[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate((train_data[:i * num_val_samples],\n",
    "                                         train_data[(i + 1) * num_val_samples:]), axis=0)\n",
    "    partial_train_label = np.concatenate([train_label[:i * num_val_samples],\n",
    "                                         train_label[(i + 1) * num_val_samples:]], axis=0)\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data,partial_train_label,epochs=num_epochs,\n",
    "              validation_data=(val_data,val_label),batch_size=30,verbose=1)\n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_history.append(mae_history)\n",
    "\n",
    "#所有验证集在每一个轮次的平均绝对误差的平均数的列表\n",
    "average_mae = [np.mean([x[i] for x in all_mae_history]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def epochs_mae(mae):\n",
    "    plt.plot(range(1,len(mae)+1),mae)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('validation mae')\n",
    "    plt.show()\n",
    "#平滑处理\n",
    "smooth_points = []\n",
    "factor = 0.9\n",
    "for point in average_mae[10:]:\n",
    "    if smooth_points:\n",
    "        previous = smooth_points[-1]\n",
    "        smooth_points.append(previous*factor + point*(1-factor))\n",
    "    else:\n",
    "        smooth_points.append(point)\n",
    "epochs_mae(smooth_points)\n",
    "smooth_points2 = []\n",
    "last_val = all_mae_history[-1]\n",
    "for point in last_val:\n",
    "    if smooth_points2:\n",
    "        previous = smooth_points2[-1]\n",
    "        smooth_points2.append(previous*factor + point*(1-factor))\n",
    "    else:\n",
    "        smooth_points2.append(point)\n",
    "epochs_mae(smooth_points2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3416/3416 [==============================] - 0s 33us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1096151.6416861827, 741.154296875]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
